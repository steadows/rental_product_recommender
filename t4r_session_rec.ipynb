{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb1736f",
   "metadata": {},
   "source": [
    "# Session-Based Recommendation with Transformers4Rec\n",
    "\n",
    "This notebook implements a session-based recommender system using NVIDIA's [Transformers4Rec](https://github.com/NVIDIA-Merlin/Transformers4Rec) library.\n",
    "\n",
    "We will:\n",
    "1.  **Setup**: Install necessary libraries.\n",
    "2.  **Preprocess**: Use NVTabular to create session sequences from our rental data.\n",
    "3.  **Model**: Define a Transformer-based model (e.g., XLNet).\n",
    "4.  **Train**: Train the model to predict the next item in a session.\n",
    "5.  **Evaluate**: Check performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e140936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'\n",
      "  warn(f\"Triton dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import transformers4rec.torch as tr\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f693b1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing with NVTabular\n",
    "\n",
    "We need to transform our raw interaction data into a format suitable for sequential models.\n",
    "This involves:\n",
    "1.  Loading the raw data (Hits and Visits).\n",
    "2.  Merging them to associate products with sessions.\n",
    "3.  Using NVTabular to group interactions by session (`visit_id`) and create sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ba8e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enriched interactions...\n",
      "Loaded 331689 interactions.\n",
      "Columns: ['visit_id', 'item_id', 'date_time', 'traffic_source', 'region_city', 'brand', 'main_category', 'price_bucket', 'hour', 'day_of_week', 'is_weekend', 'device_category', 'mobile_phone', 'item_popularity', 'category_popularity', 'conversion_rate', 'user_session_rank', 'days_since_last_session', 'is_new_user']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>region_city</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_category</th>\n",
       "      <th>price_bucket</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>device_category</th>\n",
       "      <th>mobile_phone</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>category_popularity</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>user_session_rank</th>\n",
       "      <th>days_since_last_session</th>\n",
       "      <th>is_new_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7155825714110136555</td>\n",
       "      <td>495491082</td>\n",
       "      <td>2023-12-02 01:35:08</td>\n",
       "      <td>direct</td>\n",
       "      <td>Ivanteevka</td>\n",
       "      <td>Weina</td>\n",
       "      <td>Ходунки</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>261</td>\n",
       "      <td>2302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2091271487556157575</td>\n",
       "      <td>463480491</td>\n",
       "      <td>2022-04-22 10:59:27</td>\n",
       "      <td>social</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>4moms</td>\n",
       "      <td>Электрокачели</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>apple</td>\n",
       "      <td>7113</td>\n",
       "      <td>21831</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>573835779580362831</td>\n",
       "      <td>495257463</td>\n",
       "      <td>2022-02-14 11:03:29</td>\n",
       "      <td>organic</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Chicсo</td>\n",
       "      <td>Автокресла</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>1133</td>\n",
       "      <td>63506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>573835779580362831</td>\n",
       "      <td>495520383</td>\n",
       "      <td>2022-02-14 11:03:29</td>\n",
       "      <td>organic</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Vtech</td>\n",
       "      <td>Музыкальные инструменты</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>158</td>\n",
       "      <td>2779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>573835779580362831</td>\n",
       "      <td>495257463</td>\n",
       "      <td>2022-02-14 11:03:29</td>\n",
       "      <td>organic</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Chicсo</td>\n",
       "      <td>Автокресла</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>1133</td>\n",
       "      <td>63506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              visit_id    item_id           date_time traffic_source  \\\n",
       "0  7155825714110136555  495491082 2023-12-02 01:35:08         direct   \n",
       "1  2091271487556157575  463480491 2022-04-22 10:59:27         social   \n",
       "2   573835779580362831  495257463 2022-02-14 11:03:29        organic   \n",
       "3   573835779580362831  495520383 2022-02-14 11:03:29        organic   \n",
       "4   573835779580362831  495257463 2022-02-14 11:03:29        organic   \n",
       "\n",
       "  region_city   brand            main_category  price_bucket  hour  \\\n",
       "0  Ivanteevka   Weina                  Ходунки             2     1   \n",
       "1      Moscow   4moms            Электрокачели             6    10   \n",
       "2      Moscow  Chicсo               Автокресла             2    11   \n",
       "3      Moscow   Vtech  Музыкальные инструменты             0    11   \n",
       "4      Moscow  Chicсo               Автокресла             2    11   \n",
       "\n",
       "   day_of_week  is_weekend  device_category mobile_phone  item_popularity  \\\n",
       "0            5           1                1         None              261   \n",
       "1            4           0                2        apple             7113   \n",
       "2            0           0                2       xiaomi             1133   \n",
       "3            0           0                2       xiaomi              158   \n",
       "4            0           0                2       xiaomi             1133   \n",
       "\n",
       "   category_popularity  conversion_rate  user_session_rank  \\\n",
       "0                 2302         0.000000                  1   \n",
       "1                21831         0.000141                  1   \n",
       "2                63506         0.000000                  1   \n",
       "3                 2779         0.000000                  1   \n",
       "4                63506         0.000000                  1   \n",
       "\n",
       "   days_since_last_session  is_new_user  \n",
       "0                     -1.0            1  \n",
       "1                     -1.0            1  \n",
       "2                     -1.0            1  \n",
       "3                     -1.0            1  \n",
       "4                     -1.0            1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load Enriched Data\n",
    "# We use the feature engineering notebook to prepare the data\n",
    "# This file contains: visit_id, item_id, context features, item metadata, and counter features.\n",
    "\n",
    "print(\"Loading enriched interactions...\")\n",
    "interactions = pd.read_parquet('data/enriched_interactions.parquet')\n",
    "\n",
    "print(f\"Loaded {len(interactions)} interactions.\")\n",
    "print(\"Columns:\", interactions.columns.tolist())\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a3c55d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming with NVTabular...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTabular processing complete.\n"
     ]
    }
   ],
   "source": [
    "# 2. Define NVTabular Workflow\n",
    "# We will create a workflow to:\n",
    "# - Categorify categorical features\n",
    "# - Normalize continuous features\n",
    "# - Group by visit_id to create sequences\n",
    "\n",
    "# Define Feature Columns\n",
    "# Categorical Features\n",
    "item_id = ['item_id'] >> Categorify(dtype=\"int64\") >> TagAsItemID()\n",
    "traffic_source = ['traffic_source'] >> Categorify(dtype=\"int64\")\n",
    "region_city = ['region_city'] >> Categorify(dtype=\"int64\")\n",
    "brand = ['brand'] >> Categorify(dtype=\"int64\")\n",
    "main_category = ['main_category'] >> Categorify(dtype=\"int64\")\n",
    "price_bucket = ['price_bucket'] >> Categorify(dtype=\"int64\")\n",
    "hour = ['hour'] >> Categorify(dtype=\"int64\")\n",
    "day_of_week = ['day_of_week'] >> Categorify(dtype=\"int64\")\n",
    "is_weekend = ['is_weekend'] >> Categorify(dtype=\"int64\")\n",
    "\n",
    "# Device Features\n",
    "device_category = ['device_category'] >> Categorify(dtype=\"int64\")\n",
    "mobile_phone = ['mobile_phone'] >> Categorify(dtype=\"int64\")\n",
    "\n",
    "# User History (Categorical)\n",
    "is_new_user = ['is_new_user'] >> Categorify(dtype=\"int64\")\n",
    "\n",
    "# Continuous Features (Counters)\n",
    "# We LogOp then Normalize to handle skewed distributions typical of popularity\n",
    "item_popularity = ['item_popularity'] >> LogOp() >> Normalize()\n",
    "category_popularity = ['category_popularity'] >> LogOp() >> Normalize()\n",
    "\n",
    "# Conversion Rate (already 0-1 range, just normalize)\n",
    "conversion_rate = ['conversion_rate'] >> Normalize()\n",
    "\n",
    "# User History (Continuous)\n",
    "user_session_rank = ['user_session_rank'] >> LogOp() >> Normalize()\n",
    "days_since_last_session = ['days_since_last_session'] >> FillMissing() >> Normalize()\n",
    "\n",
    "session_id = ['visit_id'] >> Categorify(dtype=\"int64\") >> TagAsUserID()\n",
    "time_col = ['date_time']\n",
    "\n",
    "# Grouping to create sequences\n",
    "# We group by 'visit_id' and aggregate other columns into lists\n",
    "groupby_features = (\n",
    "    session_id + item_id + traffic_source + region_city + \n",
    "    brand + main_category + price_bucket + \n",
    "    hour + day_of_week + is_weekend +\n",
    "    device_category + mobile_phone +  # Device\n",
    "    is_new_user +  # User History\n",
    "    item_popularity + category_popularity +\n",
    "    conversion_rate +  # Item Rate\n",
    "    user_session_rank + days_since_last_session +  # User History\n",
    "    time_col\n",
    ") >> Groupby(\n",
    "    groupby_cols=['visit_id'],\n",
    "    sort_cols=['date_time'],\n",
    "    aggs={\n",
    "        'item_id': 'list',\n",
    "        'traffic_source': 'list',\n",
    "        'region_city': 'list',\n",
    "        'brand': 'list',\n",
    "        'main_category': 'list',\n",
    "        'price_bucket': 'list',\n",
    "        'hour': 'list',\n",
    "        'day_of_week': 'list',\n",
    "        'is_weekend': 'list',\n",
    "        'device_category': 'list',  # Device\n",
    "        'mobile_phone': 'list',  # Device\n",
    "        'is_new_user': 'list',  # User History\n",
    "        'item_popularity': 'list',\n",
    "        'category_popularity': 'list',\n",
    "        'conversion_rate': 'list',  # Item Rate\n",
    "        'user_session_rank': 'list',  # User History\n",
    "        'days_since_last_session': 'list',  # User History\n",
    "        'date_time': 'first'\n",
    "    },\n",
    "    name_sep=\"-\"\n",
    ")\n",
    "\n",
    "workflow = nvt.Workflow(groupby_features)\n",
    "\n",
    "# Create a dataset from the pandas dataframe\n",
    "interactions = interactions.reset_index(drop=True)\n",
    "dataset = nvt.Dataset(interactions)\n",
    "\n",
    "# Fit and Transform\n",
    "print(\"Fitting and transforming with NVTabular...\")\n",
    "workflow.fit(dataset)\n",
    "workflow.transform(dataset).to_parquet(\"data/processed_sessions\")\n",
    "\n",
    "print(\"NVTabular processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41b217",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation\n",
    "\n",
    "We load the processed Parquet files into a Merlin Dataset, which T4Rec uses.\n",
    "We also define the schema, which tells the model which features are categorical, which is the item ID, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "66058b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: [{'name': 'visit_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.visit_id.parquet', 'domain': {'min': 0, 'max': 119419, 'name': 'visit_id'}, 'embedding_sizes': {'cardinality': 119420, 'dimension': 512}}, 'dtype': DType(name='uint64', element_type=<ElementType.UInt: 'uint'>, element_size=64, element_unit=None, signed=None, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.item_id.parquet', 'domain': {'min': 0, 'max': 594, 'name': 'item_id'}, 'embedding_sizes': {'cardinality': 595, 'dimension': 57}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'traffic_source-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.traffic_source.parquet', 'domain': {'min': 0, 'max': 13, 'name': 'traffic_source'}, 'embedding_sizes': {'cardinality': 14, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'region_city-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.region_city.parquet', 'domain': {'min': 0, 'max': 1176, 'name': 'region_city'}, 'embedding_sizes': {'cardinality': 1177, 'dimension': 84}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'brand-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.brand.parquet', 'domain': {'min': 0, 'max': 175, 'name': 'brand'}, 'embedding_sizes': {'cardinality': 176, 'dimension': 29}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'main_category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.main_category.parquet', 'domain': {'min': 0, 'max': 81, 'name': 'main_category'}, 'embedding_sizes': {'cardinality': 82, 'dimension': 19}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'price_bucket-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.price_bucket.parquet', 'domain': {'min': 0, 'max': 11, 'name': 'price_bucket'}, 'embedding_sizes': {'cardinality': 12, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'hour-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.hour.parquet', 'domain': {'min': 0, 'max': 26, 'name': 'hour'}, 'embedding_sizes': {'cardinality': 27, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'day_of_week-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.day_of_week.parquet', 'domain': {'min': 0, 'max': 9, 'name': 'day_of_week'}, 'embedding_sizes': {'cardinality': 10, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'is_weekend-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.is_weekend.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'is_weekend'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'device_category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.device_category.parquet', 'domain': {'min': 0, 'max': 6, 'name': 'device_category'}, 'embedding_sizes': {'cardinality': 7, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'mobile_phone-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.mobile_phone.parquet', 'domain': {'min': 0, 'max': 61, 'name': 'mobile_phone'}, 'embedding_sizes': {'cardinality': 62, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'is_new_user-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.is_new_user.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'is_new_user'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'item_popularity-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'category_popularity-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'conversion_rate-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'user_session_rank-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'days_since_last_session-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'date_time-first', 'tags': set(), 'properties': {}, 'dtype': DType(name='datetime64[ns]', element_type=<ElementType.DateTime: 'datetime'>, element_size=64, element_unit=<ElementUnit.Nanosecond: 'nanosecond'>, signed=None, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sessions: 95533\n",
      "Valid sessions: 23884\n"
     ]
    }
   ],
   "source": [
    "# Load the processed data\n",
    "# In a real scenario, we would split by time before this step.\n",
    "# For this example, we'll just load the single file and split it manually or use a subset.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "processed_path = \"data/processed_sessions\"\n",
    "schema = workflow.output_schema\n",
    "\n",
    "# Check the schema\n",
    "print(\"Schema:\", schema)\n",
    "\n",
    "# Create a Dataset\n",
    "# We can use the Merlin Dataset API\n",
    "import merlin.io\n",
    "ds = merlin.io.Dataset(processed_path, engine=\"parquet\")\n",
    "\n",
    "# Simple Time-based Split (approximate for this example)\n",
    "# We'll just take the last 20% of rows as validation since we sorted by time implicitly? \n",
    "# Actually, we should sort the parquet file or split it properly.\n",
    "# Let's assume random split for the mechanics of this demo if time split is hard to do on the fly.\n",
    "# But for session rec, time split is crucial.\n",
    "\n",
    "# Let's reload as DataFrame to split, then save back to parquet for T4Rec (easiest for small data)\n",
    "# We use glob to explicitly select .parquet files and avoid reading metadata files (like schema.pbtxt)\n",
    "parquet_files = glob.glob(os.path.join(processed_path, \"*.parquet\"))\n",
    "df = pd.read_parquet(parquet_files)\n",
    "df = df.sort_values('date_time-first')\n",
    "\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:split_index]\n",
    "valid_df = df.iloc[split_index:]\n",
    "\n",
    "train_df.to_parquet(\"data/train.parquet\")\n",
    "valid_df.to_parquet(\"data/valid.parquet\")\n",
    "\n",
    "print(f\"Train sessions: {len(train_df)}\")\n",
    "print(f\"Valid sessions: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5065a2a",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "We define the Transformer model.\n",
    "We use `SequentialBlock` to combine:\n",
    "1.  **Embeddings**: For items and side info (city, source).\n",
    "2.  **Transformer Body**: XLNet or similar.\n",
    "3.  **Prediction Head**: To predict the next item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9f84b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HYPERPARAMETERS\n",
      "==================================================\n",
      "  d_model: 64\n",
      "  n_head: 4\n",
      "  n_layer: 1\n",
      "  max_seq_length: 128\n",
      "  learning_rate: 0.0001\n",
      "  batch_size: 512\n",
      "  max_steps: -1\n",
      "  dropout: 0.5\n",
      "  summary_dropout: 0.5\n",
      "  num_features: 17\n",
      "  num_categorical: 12\n",
      "  num_continuous: 5\n",
      "  weight_decay: 0.01\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# HYPERPARAMETERS - Change values here!\n",
    "# ============================================\n",
    "hparams = {\n",
    "    # Model Architecture\n",
    "    \"d_model\": 64,\n",
    "    \"n_head\": 4,\n",
    "    \"n_layer\": 1,\n",
    "    \"max_seq_length\": 128,\n",
    "    # Training\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"batch_size\": 512,\n",
    "    \"max_steps\": -1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"summary_dropout\": 0.5,\n",
    "    # Data (auto-calculated below)\n",
    "    \"num_features\": 17,\n",
    "    \"num_categorical\": 12,\n",
    "    \"num_continuous\": 5,\n",
    "    \"weight_decay\": 0.01,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYPERPARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "for k, v in hparams.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac89f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema (Fixed): [{'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ID: 'id'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.item_id.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 57.0, 'cardinality': 595.0}, 'domain': {'min': 0, 'max': 594, 'name': 'item_id'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'traffic_source-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.traffic_source.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 14.0}, 'domain': {'min': 0, 'max': 13, 'name': 'traffic_source'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'region_city-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.region_city.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 84.0, 'cardinality': 1177.0}, 'domain': {'min': 0, 'max': 1176, 'name': 'region_city'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'brand-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.brand.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 29.0, 'cardinality': 176.0}, 'domain': {'min': 0, 'max': 175, 'name': 'brand'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'main_category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.main_category.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 19.0, 'cardinality': 82.0}, 'domain': {'min': 0, 'max': 81, 'name': 'main_category'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'price_bucket-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.price_bucket.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 12.0}, 'domain': {'min': 0, 'max': 11, 'name': 'price_bucket'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'hour-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.hour.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 27.0}, 'domain': {'min': 0, 'max': 26, 'name': 'hour'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'day_of_week-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.day_of_week.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 10.0}, 'domain': {'min': 0, 'max': 9, 'name': 'day_of_week'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'is_weekend-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.is_weekend.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'domain': {'min': 0, 'max': 4, 'name': 'is_weekend'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'device_category-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.device_category.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 7.0}, 'domain': {'min': 0, 'max': 6, 'name': 'device_category'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'mobile_phone-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.mobile_phone.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 62.0}, 'domain': {'min': 0, 'max': 61, 'name': 'mobile_phone'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'item_popularity-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'category_popularity-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'conversion_rate-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'user_session_rank-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'days_since_last_session-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>}, 'properties': {'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}, {'name': 'is_new_user-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.is_new_user.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 5.0}, 'domain': {'min': 0, 'max': 4, 'name': 'is_new_user'}, 'value_count': {'min': 0, 'max': 128}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=128)))), 'is_list': True, 'is_ragged': True}]\n",
      "\n",
      "Total input features: 17\n",
      "Model Switched: GPT-2 Architecture | Inputs: 17 features\n",
      "Model(\n",
      "  (heads): ModuleList(\n",
      "    (0): Head(\n",
      "      (body): SequentialBlock(\n",
      "        (0): TabularSequenceFeatures(\n",
      "          (_aggregation): ConcatFeatures()\n",
      "          (to_merge): ModuleDict(\n",
      "            (continuous_module): ContinuousFeatures(\n",
      "              (filter_features): FilterFeatures()\n",
      "            )\n",
      "            (categorical_module): SequenceEmbeddingFeatures(\n",
      "              (filter_features): FilterFeatures()\n",
      "              (embedding_tables): ModuleDict(\n",
      "                (item_id-list): Embedding(595, 64, padding_idx=0)\n",
      "                (traffic_source-list): Embedding(14, 64, padding_idx=0)\n",
      "                (region_city-list): Embedding(1177, 64, padding_idx=0)\n",
      "                (brand-list): Embedding(176, 64, padding_idx=0)\n",
      "                (main_category-list): Embedding(82, 64, padding_idx=0)\n",
      "                (price_bucket-list): Embedding(12, 64, padding_idx=0)\n",
      "                (hour-list): Embedding(27, 64, padding_idx=0)\n",
      "                (day_of_week-list): Embedding(10, 64, padding_idx=0)\n",
      "                (is_weekend-list): Embedding(5, 64, padding_idx=0)\n",
      "                (device_category-list): Embedding(7, 64, padding_idx=0)\n",
      "                (mobile_phone-list): Embedding(62, 64, padding_idx=0)\n",
      "                (is_new_user-list): Embedding(5, 64, padding_idx=0)\n",
      "              )\n",
      "            )\n",
      "            (pretrained_embedding_module): PretrainedEmbeddingFeatures(\n",
      "              (filter_features): FilterFeatures()\n",
      "            )\n",
      "          )\n",
      "          (projection_module): SequentialBlock(\n",
      "            (0): DenseBlock(\n",
      "              (0): Linear(in_features=773, out_features=64, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (_masking): CausalLanguageModeling()\n",
      "        )\n",
      "        (1): TansformerBlock(\n",
      "          (transformer): GPT2Model(\n",
      "            (wte): Embedding(1, 64)\n",
      "            (wpe): Embedding(1024, 64)\n",
      "            (drop): Dropout(p=0.5, inplace=False)\n",
      "            (h): ModuleList(\n",
      "              (0): GPT2Block(\n",
      "                (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): GPT2Attention(\n",
      "                  (c_attn): Conv1D()\n",
      "                  (c_proj): Conv1D()\n",
      "                  (attn_dropout): Dropout(p=0.5, inplace=False)\n",
      "                  (resid_dropout): Dropout(p=0.5, inplace=False)\n",
      "                )\n",
      "                (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): GPT2MLP(\n",
      "                  (c_fc): Conv1D()\n",
      "                  (c_proj): Conv1D()\n",
      "                  (act): GELUActivation()\n",
      "                  (dropout): Dropout(p=0.5, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (masking): CausalLanguageModeling()\n",
      "          (prepare_module): GPT2Prepare(\n",
      "            (transformer): GPT2Model(\n",
      "              (wte): Embedding(1, 64)\n",
      "              (wpe): Embedding(1024, 64)\n",
      "              (drop): Dropout(p=0.5, inplace=False)\n",
      "              (h): ModuleList(\n",
      "                (0): GPT2Block(\n",
      "                  (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                  (attn): GPT2Attention(\n",
      "                    (c_attn): Conv1D()\n",
      "                    (c_proj): Conv1D()\n",
      "                    (attn_dropout): Dropout(p=0.5, inplace=False)\n",
      "                    (resid_dropout): Dropout(p=0.5, inplace=False)\n",
      "                  )\n",
      "                  (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                  (mlp): GPT2MLP(\n",
      "                    (c_fc): Conv1D()\n",
      "                    (c_proj): Conv1D()\n",
      "                    (act): GELUActivation()\n",
      "                    (dropout): Dropout(p=0.5, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (masking): CausalLanguageModeling()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (prediction_task_dict): ModuleDict(\n",
      "        (next-item): NextItemPredictionTask(\n",
      "          (sequence_summary): SequenceSummary(\n",
      "            (summary): Identity()\n",
      "            (activation): Identity()\n",
      "            (first_dropout): Identity()\n",
      "            (last_dropout): Identity()\n",
      "          )\n",
      "          (metrics): ModuleList(\n",
      "            (0): RecallAt()\n",
      "            (1): NDCGAt()\n",
      "          )\n",
      "          (loss): CrossEntropyLoss()\n",
      "          (embeddings): SequenceEmbeddingFeatures(\n",
      "            (filter_features): FilterFeatures()\n",
      "            (embedding_tables): ModuleDict(\n",
      "              (item_id-list): Embedding(595, 64, padding_idx=0)\n",
      "              (traffic_source-list): Embedding(14, 64, padding_idx=0)\n",
      "              (region_city-list): Embedding(1177, 64, padding_idx=0)\n",
      "              (brand-list): Embedding(176, 64, padding_idx=0)\n",
      "              (main_category-list): Embedding(82, 64, padding_idx=0)\n",
      "              (price_bucket-list): Embedding(12, 64, padding_idx=0)\n",
      "              (hour-list): Embedding(27, 64, padding_idx=0)\n",
      "              (day_of_week-list): Embedding(10, 64, padding_idx=0)\n",
      "              (is_weekend-list): Embedding(5, 64, padding_idx=0)\n",
      "              (device_category-list): Embedding(7, 64, padding_idx=0)\n",
      "              (mobile_phone-list): Embedding(62, 64, padding_idx=0)\n",
      "              (is_new_user-list): Embedding(5, 64, padding_idx=0)\n",
      "            )\n",
      "          )\n",
      "          (item_embedding_table): Embedding(595, 64, padding_idx=0)\n",
      "          (masking): CausalLanguageModeling()\n",
      "          (pre): Block(\n",
      "            (module): NextItemPredictionTask(\n",
      "              (item_embedding_table): Embedding(595, 64, padding_idx=0)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the Schema for the model\n",
    "from merlin.schema import Schema, Tags\n",
    "import merlin.io\n",
    "\n",
    "# Load schema from processed data\n",
    "train_schema = merlin.io.Dataset(\"data/processed_sessions\", engine=\"parquet\").schema\n",
    "\n",
    "# Select features - ALL 19 engineered features (minus visit_id, item_id, date_time which are IDs/meta)\n",
    "# 17 model input features total\n",
    "selected_features = [\n",
    "    'item_id-list', \n",
    "    # Session Context (2)\n",
    "    'traffic_source-list', 'region_city-list',\n",
    "    # Item Metadata (3)\n",
    "    'brand-list', 'main_category-list', 'price_bucket-list',\n",
    "    # Temporal (3)\n",
    "    'hour-list', 'day_of_week-list', 'is_weekend-list',\n",
    "    # Device (2)\n",
    "    'device_category-list', 'mobile_phone-list',\n",
    "    # Continuous Counters (2)\n",
    "    'item_popularity-list', 'category_popularity-list',\n",
    "    # Item Rate (1)\n",
    "    'conversion_rate-list',\n",
    "    # User History (3)\n",
    "    'user_session_rank-list', 'days_since_last_session-list', 'is_new_user-list'\n",
    "]\n",
    "\n",
    "input_schema = train_schema.select_by_name(selected_features)\n",
    "\n",
    "# WORKAROUND: Fix schema value_counts\n",
    "new_cols = []\n",
    "for col in input_schema:\n",
    "    props = col.properties.copy()\n",
    "    if 'value_count' not in props:\n",
    "        props['value_count'] = {}\n",
    "    props['value_count']['max'] = hparams[\"max_seq_length\"]\n",
    "    new_col = col.with_properties(props)\n",
    "    new_cols.append(new_col)\n",
    "\n",
    "input_schema = Schema(new_cols)\n",
    "print(\"Input Schema (Fixed):\", input_schema)\n",
    "print(f\"\\nTotal input features: {len(selected_features)}\")\n",
    "\n",
    "# Define the Input Block (uses hparams)\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    input_schema,\n",
    "    max_sequence_length=hparams[\"max_seq_length\"],\n",
    "    aggregation=\"concat\",\n",
    "    d_output=hparams[\"d_model\"],\n",
    "    masking=\"causal\",\n",
    ")\n",
    "\n",
    "# Define the Transformer Body - XLNet (uses hparams)\n",
    "transformer_config = tr.GPT2Config.build(\n",
    "    d_model=hparams[\"d_model\"],\n",
    "    n_head=hparams[\"n_head\"],\n",
    "    n_layer=hparams[\"n_layer\"],\n",
    "    total_seq_length=1024,\n",
    "    dropout=hparams[\"dropout\"],\n",
    "    summary_dropout=hparams[\"summary_dropout\"],\n",
    ")\n",
    "\n",
    "# Define body\n",
    "body = tr.SequentialBlock(\n",
    "    input_module,\n",
    "    tr.TransformerBlock(transformer_config, masking=input_module.masking)\n",
    ")\n",
    "\n",
    "# Define ranking metrics\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n",
    "metrics = [\n",
    "    RecallAt(top_ks=[6, 10], labels_onehot=True),\n",
    "    NDCGAt(top_ks=[6, 10], labels_onehot=True)\n",
    "]\n",
    "\n",
    "# Define the Head\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, metrics=metrics),\n",
    "    inputs=input_module,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model\n",
    "model = tr.Model(head)\n",
    "\n",
    "print(f\"Model Switched: GPT-2 Architecture | Inputs: {len(selected_features)} features\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87cfb4",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We use the `Trainer` class (based on HuggingFace Trainer) to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d4ba147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "TensorBoard: Run 'tensorboard --logdir ./tb_logs' to monitor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc5fce3a0224d088837433e03d40914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1454, 'learning_rate': 9.46524064171123e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338d5d154e9d4768867f4c9f6af00d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.9507317543029785, 'eval_runtime': 2.705, 'eval_samples_per_second': 8896.173, 'eval_steps_per_second': 17.375, 'epoch': 0.53}\n",
      "{'loss': 5.655, 'learning_rate': 8.930481283422461e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577c3d8584b64e61bf634ee2a7b3cf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.671504974365234, 'eval_runtime': 2.6341, 'eval_samples_per_second': 9135.572, 'eval_steps_per_second': 17.843, 'epoch': 1.07}\n",
      "{'loss': 5.3488, 'learning_rate': 8.39572192513369e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c5552505e0483ea678b626f24c604b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.475963115692139, 'eval_runtime': 2.7012, 'eval_samples_per_second': 8908.488, 'eval_steps_per_second': 17.399, 'epoch': 1.6}\n",
      "{'loss': 5.1086, 'learning_rate': 7.86096256684492e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f821b24801014911a2429cd414ffc83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.314854621887207, 'eval_runtime': 2.7689, 'eval_samples_per_second': 8690.85, 'eval_steps_per_second': 16.974, 'epoch': 2.14}\n",
      "{'loss': 4.9481, 'learning_rate': 7.326203208556151e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e5bf9d09bc4ca4ad69409cef9159be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.192490577697754, 'eval_runtime': 2.6821, 'eval_samples_per_second': 8971.964, 'eval_steps_per_second': 17.523, 'epoch': 2.67}\n",
      "{'loss': 4.8125, 'learning_rate': 6.79144385026738e-05, 'epoch': 3.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb9c65bbb854877ba15d04e9a1d25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.090116024017334, 'eval_runtime': 2.7169, 'eval_samples_per_second': 8857.287, 'eval_steps_per_second': 17.299, 'epoch': 3.21}\n",
      "{'loss': 4.6954, 'learning_rate': 6.25668449197861e-05, 'epoch': 3.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d23f4b0c2749d1ad85d3b6f78b4c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.0207648277282715, 'eval_runtime': 2.7308, 'eval_samples_per_second': 8811.957, 'eval_steps_per_second': 17.211, 'epoch': 3.74}\n",
      "{'loss': 4.6114, 'learning_rate': 5.721925133689839e-05, 'epoch': 4.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8361af1a04242cbb9a6dfb12b49142f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.965872764587402, 'eval_runtime': 2.7459, 'eval_samples_per_second': 8763.598, 'eval_steps_per_second': 17.116, 'epoch': 4.28}\n",
      "{'loss': 4.5323, 'learning_rate': 5.1871657754010694e-05, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860b15d0b43748b5a7d7ec27eb715353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.907406806945801, 'eval_runtime': 2.6877, 'eval_samples_per_second': 8953.256, 'eval_steps_per_second': 17.487, 'epoch': 4.81}\n",
      "{'loss': 4.4747, 'learning_rate': 4.6524064171123e-05, 'epoch': 5.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5cae24abb84fc5b274a4bf0562e42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.858163833618164, 'eval_runtime': 2.8206, 'eval_samples_per_second': 8531.449, 'eval_steps_per_second': 16.663, 'epoch': 5.35}\n",
      "{'loss': 4.4265, 'learning_rate': 4.11764705882353e-05, 'epoch': 5.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6761e8116e9045e28683044d60f89f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.835385799407959, 'eval_runtime': 2.7655, 'eval_samples_per_second': 8701.651, 'eval_steps_per_second': 16.995, 'epoch': 5.88}\n",
      "{'loss': 4.3826, 'learning_rate': 3.582887700534759e-05, 'epoch': 6.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065d54c0583643bf9733963a36cfb186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.808504104614258, 'eval_runtime': 2.8031, 'eval_samples_per_second': 8584.872, 'eval_steps_per_second': 16.767, 'epoch': 6.42}\n",
      "{'loss': 4.3497, 'learning_rate': 3.0481283422459894e-05, 'epoch': 6.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96992f650aa642fa956200a332e05e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.768310070037842, 'eval_runtime': 2.779, 'eval_samples_per_second': 8659.136, 'eval_steps_per_second': 16.912, 'epoch': 6.95}\n",
      "{'loss': 4.3044, 'learning_rate': 2.5133689839572196e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11975be4dc5e421c97b4dbfac80e4164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.738343715667725, 'eval_runtime': 2.6854, 'eval_samples_per_second': 8960.932, 'eval_steps_per_second': 17.502, 'epoch': 7.49}\n",
      "{'loss': 4.3018, 'learning_rate': 1.9786096256684494e-05, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874587009d784b68b31cf6a270e5fb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.733992099761963, 'eval_runtime': 2.8636, 'eval_samples_per_second': 8403.335, 'eval_steps_per_second': 16.413, 'epoch': 8.02}\n",
      "{'loss': 4.2748, 'learning_rate': 1.4438502673796791e-05, 'epoch': 8.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587d2591ef914bea984679b2d0ff809d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.714770317077637, 'eval_runtime': 2.9005, 'eval_samples_per_second': 8296.526, 'eval_steps_per_second': 16.204, 'epoch': 8.56}\n",
      "{'loss': 4.2651, 'learning_rate': 9.090909090909091e-06, 'epoch': 9.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d049c325f74c8881fa0126540930a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.711856842041016, 'eval_runtime': 2.7196, 'eval_samples_per_second': 8848.373, 'eval_steps_per_second': 17.282, 'epoch': 9.09}\n",
      "{'loss': 4.2473, 'learning_rate': 3.7433155080213903e-06, 'epoch': 9.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba9a4c7f21a496da71ed492355263fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 4.702125549316406, 'eval_runtime': 2.9268, 'eval_samples_per_second': 8221.901, 'eval_steps_per_second': 16.058, 'epoch': 9.63}\n",
      "{'train_runtime': 319.8618, 'train_samples_per_second': 2993.293, 'train_steps_per_second': 5.846, 'train_loss': 4.698595359108665, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1870, training_loss=4.698595359108665, metrics={'train_runtime': 319.8618, 'train_samples_per_second': 2993.293, 'train_steps_per_second': 5.846, 'total_flos': 0.0, 'train_loss': 4.698595359108665})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers4rec.torch.trainer import Trainer\n",
    "from transformers4rec.torch.utils.data_utils import MerlinDataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Training Arguments with TensorBoard (uses hparams from above)\n",
    "training_args = tr.T4RecTrainingArguments(\n",
    "    output_dir=\"./t4r_output\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=hparams[\"learning_rate\"],\n",
    "    per_device_train_batch_size=hparams[\"batch_size\"],\n",
    "    per_device_eval_batch_size=hparams[\"batch_size\"],\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # TensorBoard\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=\"./tb_logs\",\n",
    "    # Best model tracking workaround: use eval_loss for checkpointing\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_/loss\",\n",
    "    greater_is_better=False,\n",
    "    # Other\n",
    "    dataloader_drop_last=False,\n",
    "    compute_metrics_each_n_steps=1,\n",
    "    use_mps_device=False,\n",
    "    no_cuda=True,\n",
    "    weight_decay=hparams[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=train_schema,\n",
    "    train_dataset_or_path=\"data/train.parquet\",\n",
    "    eval_dataset_or_path=\"data/valid.parquet\",\n",
    ")\n",
    "\n",
    "# Log hyperparameters to TensorBoard\n",
    "tb_writer = SummaryWriter(log_dir=\"./tb_logs\")\n",
    "tb_writer.add_hparams(hparams, {\"placeholder\": 0})\n",
    "tb_writer.close()\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "print(\"TensorBoard: Run 'tensorboard --logdir ./tb_logs' to monitor\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcef1f",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We evaluate the model using ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7787b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87753db05f4c4663866928749e644df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Evaluation Metrics:\n",
      "eval_/next-item/recall_at_6: 0.3113028407096863\n",
      "eval_/next-item/recall_at_10: 0.3574920892715454\n",
      "eval_/next-item/ndcg_at_6: 0.23176944255828857\n",
      "eval_/next-item/ndcg_at_10: 0.24618665874004364\n",
      "eval_/loss: 6.106310844421387\n",
      "eval_runtime: 3.5025\n",
      "eval_samples_per_second: 8039.977\n",
      "eval_steps_per_second: 15.703\n",
      "\n",
      "Filtered Results:\n",
      "eval_/next-item/recall_at_6: 0.3113\n",
      "eval_/next-item/recall_at_10: 0.3575\n",
      "eval_/next-item/ndcg_at_6: 0.2318\n",
      "eval_/next-item/ndcg_at_10: 0.2462\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Enable metric computation (it defaults to None in Trainer)\n",
    "trainer.compute_metrics = True\n",
    "\n",
    "# Note: The trainer already has the eval_dataset_or_path from initialization\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nAll Evaluation Metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nFiltered Results:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    if \"recall\" in key or \"ndcg\" in key:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "053e94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enriched test data...\n",
      "Mapping Test Data to Integer IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Aligned and Saved to 'data/processed_test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Enriched Test Data you just created\n",
    "print(\"Loading enriched test data...\")\n",
    "raw_test_dataset = nvt.Dataset(\"data/enriched_interactions_test.parquet\")\n",
    "\n",
    "# 2. Transform using the TRAINING workflow\n",
    "# CRITICAL: We use 'workflow.transform', NOT 'workflow.fit'\n",
    "# This ensures \"Nike\" is mapped to ID 105, just like in training.\n",
    "print(\"Mapping Test Data to Integer IDs...\")\n",
    "processed_test = workflow.transform(raw_test_dataset)\n",
    "\n",
    "# 3. Save to Processed Parquet\n",
    "processed_test.to_parquet(\"data/processed_test\")\n",
    "print(\"Test Data Aligned and Saved to 'data/processed_test'\")\n",
    "\n",
    "# 4. Generate Submission\n",
    "# Now you can run the prediction code from my previous response!\n",
    "test_dataset_final = merlin.io.Dataset(\"data/processed_test\", engine=\"parquet\")\n",
    "# trainer.predict(test_dataset_final) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40bd7f",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ddd17b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aligned test data for inference...\n",
      "Running model inference (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73834c6c806b4271a284d66a259e0761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (2062, 100)\n",
      "Loading Item ID mapping...\n",
      "Loaded 592 item categories.\n",
      "Mapping IDs and formatting submission...\n",
      "Reading visit IDs from test file...\n",
      "\n",
      "SUCCESS! Saved intermediate 'submission.csv'\n",
      "              visit_id                                        product_ids\n",
      "0  3705189088312688779  495255340 495399681 463480423 495370340 495401...\n",
      "1  3705579516103753793  495402312 495400605 495402322 495254783 495254...\n",
      "2  3706260855703732415  495403143 495403163 495400474 495256989 463480...\n",
      "3  3706380385759789156  463480699 495401895 495277293 495370340 495493...\n",
      "4  3706985600653983919  495403163 463480699 463480417 495255645 495370...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import merlin.io\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Load the Processed Test Data\n",
    "print(\"Loading aligned test data for inference...\")\n",
    "test_path = \"data/processed_test\"\n",
    "test_dataset = merlin.io.Dataset(test_path, engine=\"parquet\")\n",
    "\n",
    "# 2. Generate Predictions\n",
    "print(\"Running model inference (this may take a minute)...\")\n",
    "# Assuming 'trainer' is already defined in previous cells\n",
    "output = trainer.predict(test_dataset) \n",
    "\n",
    "# === FIX 1: Handle Tuple Output & Dimensions ===\n",
    "# Check if predictions is a tuple (common in HF/T4Rec)\n",
    "if isinstance(output.predictions, tuple):\n",
    "    logits = output.predictions[0]\n",
    "else:\n",
    "    logits = output.predictions\n",
    "\n",
    "# Handle shapes: [Batch, Seq, Vocab] -> [Batch, Vocab]\n",
    "# We take the prediction corresponding to the LAST item in the session\n",
    "if len(logits.shape) == 3:\n",
    "    final_step_logits = logits[:, -1, :]\n",
    "else:\n",
    "    final_step_logits = logits\n",
    "\n",
    "print(f\"Prediction shape: {final_step_logits.shape}\")\n",
    "\n",
    "# 3. Get Top-100 Candidates (The Buffer Strategy)\n",
    "# === CHANGED FROM 20 TO 100 ===\n",
    "K_CANDIDATES = 100 \n",
    "logits_tensor = torch.from_numpy(final_step_logits).float()\n",
    "top_scores, top_indices = torch.topk(logits_tensor, k=K_CANDIDATES, dim=1)\n",
    "top_indices = top_indices.numpy()\n",
    "\n",
    "# 4. Load the \"Decoder Ring\" (Integer -> String ID)\n",
    "print(\"Loading Item ID mapping...\")\n",
    "categories_path = \"categories/unique.item_id.parquet\"\n",
    "\n",
    "if os.path.exists(categories_path):\n",
    "    categories_df = pd.read_parquet(categories_path)\n",
    "    # Create array where index = integer ID, value = string Product ID\n",
    "    item_map = categories_df['item_id'].values\n",
    "    print(f\"Loaded {len(item_map)} item categories.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find categories file at {categories_path}\")\n",
    "\n",
    "# 5. Map Integers to Product IDs and Filter\n",
    "print(\"Mapping IDs and formatting submission...\")\n",
    "\n",
    "def is_valid_id(s_id):\n",
    "    # Filter out Padding (None/0), Unknown (<NA>), or empty strings\n",
    "    return s_id and str(s_id) not in ['0', '1', '<NA>', 'None', 'nan']\n",
    "\n",
    "final_predictions = []\n",
    "\n",
    "for row_idx, indices in enumerate(top_indices):\n",
    "    valid_items = []\n",
    "    \n",
    "    # We now have 100 candidates to look through, so we are much more likely\n",
    "    # to find 6 valid ones before running out.\n",
    "    for token_id in indices:\n",
    "        # Safety bound check\n",
    "        if token_id >= len(item_map):\n",
    "            continue\n",
    "            \n",
    "        real_id = item_map[token_id]\n",
    "        \n",
    "        if is_valid_id(real_id):\n",
    "            s_real_id = str(real_id)\n",
    "            if s_real_id not in valid_items:\n",
    "                valid_items.append(s_real_id)\n",
    "        \n",
    "        # Stop once we have 6 valid items\n",
    "        if len(valid_items) == 6:\n",
    "            break\n",
    "    \n",
    "    # FILLER STRATEGY: If < 6 items found even after checking top 100\n",
    "    while len(valid_items) < 6:\n",
    "        if len(valid_items) > 0:\n",
    "            valid_items.append(valid_items[0]) # Duplicate best item\n",
    "        else:\n",
    "            valid_items.append(\"0\") # Emergency fallback\n",
    "            \n",
    "    final_predictions.append(\" \".join(valid_items))\n",
    "\n",
    "# 6. Create Submission DataFrame\n",
    "print(\"Reading visit IDs from test file...\")\n",
    "\n",
    "# === FIX 2: Use Glob to avoid schema.pbtxt crash ===\n",
    "# Find only actual .parquet files\n",
    "parquet_files = glob.glob(os.path.join(test_path, \"*.parquet\"))\n",
    "\n",
    "# Read the specific files to get the visit_ids\n",
    "test_id_df = pd.read_parquet(parquet_files, columns=['visit_id'])\n",
    "\n",
    "# Safety Check for length mismatch\n",
    "min_len = min(len(test_id_df), len(final_predictions))\n",
    "if len(test_id_df) != len(final_predictions):\n",
    "    print(f\"WARNING: Length mismatch! IDs: {len(test_id_df)}, Preds: {len(final_predictions)}\")\n",
    "    print(\"Truncating to match the shorter length.\")\n",
    "    test_id_df = test_id_df.iloc[:min_len]\n",
    "    final_predictions = final_predictions[:min_len]\n",
    "\n",
    "# Create the dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'visit_id': test_id_df['visit_id'],\n",
    "    'product_ids': final_predictions\n",
    "})\n",
    "\n",
    "# 7. Save Intermediate Result\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSUCCESS! Saved intermediate 'submission.csv'\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a9823423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Product Catalog for ID Mapping...\n",
      "Fallback prediction for empty sessions: 463480210 463480211 463480212 463480213 463480214 463480215\n",
      "Current Predictions: 2062 rows\n",
      "Converting Slugs to Numerical IDs...\n",
      "Aligning with full Test Set...\n",
      "Required Rows: 3891\n",
      "Sessions with no prediction (filled with popular items): 1829\n",
      "Final Submission Rows: 3891\n",
      "\n",
      "SUCCESS! Saved 'submission_final.csv'\n",
      "              visit_id                                        product_ids\n",
      "0  3705073560174199024  463480210 463480211 463480212 463480213 463480...\n",
      "1  3705189088312688779  495255340 495399681 463480423 495370340 495401...\n",
      "2  3705549051029618879  463480210 463480211 463480212 463480213 463480...\n",
      "3  3705579516103753793  495402312 495400605 495402322 495254783 495254...\n",
      "4  3705717843210797336  463480210 463480211 463480212 463480213 463480...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA & MAPPER\n",
    "# ==========================================\n",
    "print(\"Loading Product Catalog for ID Mapping...\")\n",
    "products_df = pd.read_csv('data/new_site_products.csv')\n",
    "\n",
    "# === FIX 3: Map Slugs to IDs ===\n",
    "# Ensure IDs are strings\n",
    "products_df['id'] = products_df['id'].astype(str)\n",
    "# Create a dictionary to map 'slug' -> 'numerical_id'\n",
    "slug_to_id_map = dict(zip(products_df['slug'], products_df['id']))\n",
    "\n",
    "# Calculate Top 6 Popular IDs for \"Cold Start\" fallback\n",
    "# We take the first 6 IDs from the catalog as a baseline\n",
    "top_6_fallback = \" \".join(products_df['id'].head(6).tolist())\n",
    "print(f\"Fallback prediction for empty sessions: {top_6_fallback}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD YOUR CURRENT PREDICTIONS\n",
    "# ==========================================\n",
    "current_submission = pd.read_csv(\"submission.csv\")\n",
    "print(f\"Current Predictions: {len(current_submission)} rows\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. CONVERT SLUGS TO NUMERICAL IDs\n",
    "# ==========================================\n",
    "def convert_slugs_to_ids(slug_string):\n",
    "    slugs = slug_string.split(\" \")\n",
    "    ids = []\n",
    "    for slug in slugs:\n",
    "        # Map slug to ID\n",
    "        if slug in slug_to_id_map:\n",
    "            ids.append(slug_to_id_map[slug])\n",
    "        else:\n",
    "            # If mapping fails, check if it's already a number\n",
    "            if str(slug).isdigit():\n",
    "                 ids.append(str(slug))\n",
    "            \n",
    "    # If we have < 6 items after mapping, fill with fallback\n",
    "    fallback_list = top_6_fallback.split(\" \")\n",
    "    \n",
    "    while len(ids) < 6:\n",
    "        for backup_id in fallback_list:\n",
    "            if backup_id not in ids:\n",
    "                ids.append(backup_id)\n",
    "            if len(ids) == 6: break\n",
    "            \n",
    "    return \" \".join(ids[:6])\n",
    "\n",
    "print(\"Converting Slugs to Numerical IDs...\")\n",
    "current_submission['product_ids'] = current_submission['product_ids'].apply(convert_slugs_to_ids)\n",
    "\n",
    "# ==========================================\n",
    "# 4. FIX MISSING ROWS (Align with Kaggle Test List)\n",
    "# ==========================================\n",
    "print(\"Aligning with full Test Set...\")\n",
    "\n",
    "# Load the RAW test visits to get the complete list of required visit_ids\n",
    "full_test_visits = pd.read_csv('data/metrika_visits_test.csv')\n",
    "required_ids = full_test_visits[['visit_id']].drop_duplicates()\n",
    "\n",
    "print(f\"Required Rows: {len(required_ids)}\")\n",
    "\n",
    "# Merge: Left join keeps all required IDs, adds NaN where we have no prediction\n",
    "final_df = required_ids.merge(current_submission, on='visit_id', how='left')\n",
    "\n",
    "# Fill NaN (sessions that were dropped during preprocessing) with Top 6\n",
    "missing_count = final_df['product_ids'].isna().sum()\n",
    "print(f\"Sessions with no prediction (filled with popular items): {missing_count}\")\n",
    "\n",
    "final_df['product_ids'] = final_df['product_ids'].fillna(top_6_fallback)\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL SAVE\n",
    "# ==========================================\n",
    "# Ensure output format is exact\n",
    "final_df['visit_id'] = final_df['visit_id'].astype(str)\n",
    "\n",
    "print(f\"Final Submission Rows: {len(final_df)}\")\n",
    "final_df.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"\\nSUCCESS! Saved 'submission_final.csv'\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t4rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
