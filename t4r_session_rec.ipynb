{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb1736f",
   "metadata": {},
   "source": [
    "# Session-Based Recommendation with Transformers4Rec\n",
    "\n",
    "This notebook implements a session-based recommender system using NVIDIA's [Transformers4Rec](https://github.com/NVIDIA-Merlin/Transformers4Rec) library.\n",
    "\n",
    "We will:\n",
    "1.  **Setup**: Install necessary libraries.\n",
    "2.  **Preprocess**: Use NVTabular to create session sequences from our rental data.\n",
    "3.  **Model**: Define a Transformer-based model (e.g., XLNet).\n",
    "4.  **Train**: Train the model to predict the next item in a session.\n",
    "5.  **Evaluate**: Check performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e140936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'\n",
      "  warn(f\"Triton dtype mappings did not load successfully due to an error: {exc.msg}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import transformers4rec.torch as tr\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f693b1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing with NVTabular\n",
    "\n",
    "We need to transform our raw interaction data into a format suitable for sequential models.\n",
    "This involves:\n",
    "1.  Loading the raw data (Hits and Visits).\n",
    "2.  Merging them to associate products with sessions.\n",
    "3.  Using NVTabular to group interactions by session (`visit_id`) and create sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Processed interactions: 408562\n",
      "Processed interactions: 408562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>traffic_source</th>\n",
       "      <th>region_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122887</th>\n",
       "      <td>463311640199432</td>\n",
       "      <td>avtokreslo-chicco-synthesis-xt-plus</td>\n",
       "      <td>2022-01-20 03:29:26</td>\n",
       "      <td>ad</td>\n",
       "      <td>Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122896</th>\n",
       "      <td>640428033179772</td>\n",
       "      <td>manezh-krovat-capella-best-friends</td>\n",
       "      <td>2022-01-20 03:40:42</td>\n",
       "      <td>ad</td>\n",
       "      <td>Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122902</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>2022-01-20 03:45:26</td>\n",
       "      <td>direct</td>\n",
       "      <td>Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122903</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>2022-01-20 03:45:26</td>\n",
       "      <td>direct</td>\n",
       "      <td>Moscow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122906</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>2022-01-20 03:45:26</td>\n",
       "      <td>direct</td>\n",
       "      <td>Moscow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               visit_id                              item_id  \\\n",
       "122887  463311640199432  avtokreslo-chicco-synthesis-xt-plus   \n",
       "122896  640428033179772   manezh-krovat-capella-best-friends   \n",
       "122902  714740689010850                 piratskiy-korabl-elc   \n",
       "122903  714740689010850                 piratskiy-korabl-elc   \n",
       "122906  714740689010850                 piratskiy-korabl-elc   \n",
       "\n",
       "                 date_time traffic_source region_city  \n",
       "122887 2022-01-20 03:29:26             ad      Moscow  \n",
       "122896 2022-01-20 03:40:42             ad      Moscow  \n",
       "122902 2022-01-20 03:45:26         direct      Moscow  \n",
       "122903 2022-01-20 03:45:26         direct      Moscow  \n",
       "122906 2022-01-20 03:45:26         direct      Moscow  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load Enriched Data\n",
    "# We use the feature engineering notebook to prepare the data\n",
    "# This file contains: visit_id, item_id, context features, item metadata, and counter features.\n",
    "\n",
    "print(\"Loading enriched interactions...\")\n",
    "interactions = pd.read_parquet('data/enriched_interactions.parquet')\n",
    "\n",
    "print(f\"Loaded {len(interactions)} interactions.\")\n",
    "print(\"Columns:\", interactions.columns.tolist())\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c55d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming with NVTabular...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVTabular processing complete.\n"
     ]
    }
   ],
   "source": [
    "# 2. Define NVTabular Workflow\n",
    "# We will create a workflow to:\n",
    "# - Categorify categorical features\n",
    "# - Normalize continuous features\n",
    "# - Group by visit_id to create sequences\n",
    "\n",
    "# Define Feature Columns\n",
    "# Categorical Features\n",
    "item_id = ['item_id'] >> Categorify(dtype=\"int64\") >> TagAsItemID()\n",
    "traffic_source = ['traffic_source'] >> Categorify(dtype=\"int64\")\n",
    "region_city = ['region_city'] >> Categorify(dtype=\"int64\")\n",
    "brand = ['brand'] >> Categorify(dtype=\"int64\")\n",
    "main_category = ['main_category'] >> Categorify(dtype=\"int64\")\n",
    "price_bucket = ['price_bucket'] >> Categorify(dtype=\"int64\")\n",
    "hour = ['hour'] >> Categorify(dtype=\"int64\")\n",
    "day_of_week = ['day_of_week'] >> Categorify(dtype=\"int64\")\n",
    "is_weekend = ['is_weekend'] >> Categorify(dtype=\"int64\")\n",
    "\n",
    "# Continuous Features (Counters)\n",
    "# We LogOp then Normalize to handle skewed distributions typical of popularity\n",
    "item_popularity = ['item_popularity'] >> LogOp() >> Normalize()\n",
    "category_popularity = ['category_popularity'] >> LogOp() >> Normalize()\n",
    "\n",
    "session_id = ['visit_id'] >> Categorify(dtype=\"int64\") >> TagAsUserID()\n",
    "time_col = ['date_time']\n",
    "\n",
    "# Grouping to create sequences\n",
    "# We group by 'visit_id' and aggregate other columns into lists\n",
    "# Note: Context features (city, source, time) are constant per session in our logic, \n",
    "# but T4Rec expects sequences. We'll just list them and T4Rec can handle them.\n",
    "groupby_features = (\n",
    "    session_id + item_id + traffic_source + region_city + \n",
    "    brand + main_category + price_bucket + \n",
    "    hour + day_of_week + is_weekend +\n",
    "    item_popularity + category_popularity +\n",
    "    time_col\n",
    ") >> Groupby(\n",
    "    groupby_cols=['visit_id'],\n",
    "    sort_cols=['date_time'],\n",
    "    aggs={\n",
    "        'item_id': 'list',\n",
    "        'traffic_source': 'list',\n",
    "        'region_city': 'list',\n",
    "        'brand': 'list',\n",
    "        'main_category': 'list',\n",
    "        'price_bucket': 'list',\n",
    "        'hour': 'list',\n",
    "        'day_of_week': 'list',\n",
    "        'is_weekend': 'list',\n",
    "        'item_popularity': 'list',\n",
    "        'category_popularity': 'list',\n",
    "        'date_time': 'first'\n",
    "    },\n",
    "    name_sep=\"-\"\n",
    ")\n",
    "\n",
    "workflow = nvt.Workflow(groupby_features)\n",
    "\n",
    "# Create a dataset from the pandas dataframe\n",
    "interactions = interactions.reset_index(drop=True)\n",
    "dataset = nvt.Dataset(interactions)\n",
    "\n",
    "# Fit and Transform\n",
    "print(\"Fitting and transforming with NVTabular...\")\n",
    "workflow.fit(dataset)\n",
    "workflow.transform(dataset).to_parquet(\"data/processed_sessions\")\n",
    "\n",
    "print(\"NVTabular processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41b217",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation\n",
    "\n",
    "We load the processed Parquet files into a Merlin Dataset, which T4Rec uses.\n",
    "We also define the schema, which tells the model which features are categorical, which is the item ID, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66058b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: [{'name': 'visit_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.visit_id.parquet', 'domain': {'min': 0, 'max': 139158, 'name': 'visit_id'}, 'embedding_sizes': {'cardinality': 139159, 'dimension': 512}}, 'dtype': DType(name='uint64', element_type=<ElementType.UInt: 'uint'>, element_size=64, element_unit=None, signed=None, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.item_id.parquet', 'domain': {'min': 0, 'max': 1199, 'name': 'item_id'}, 'embedding_sizes': {'cardinality': 1200, 'dimension': 85}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'traffic_source-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.traffic_source.parquet', 'domain': {'min': 0, 'max': 13, 'name': 'traffic_source'}, 'embedding_sizes': {'cardinality': 14, 'dimension': 16}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'region_city-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.region_city.parquet', 'domain': {'min': 0, 'max': 1253, 'name': 'region_city'}, 'embedding_sizes': {'cardinality': 1254, 'dimension': 87}, 'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}, {'name': 'date_time-first', 'tags': set(), 'properties': {}, 'dtype': DType(name='datetime64[ns]', element_type=<ElementType.DateTime: 'datetime'>, element_size=64, element_unit=<ElementUnit.Nanosecond: 'nanosecond'>, signed=None, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sessions: 111324\n",
      "Valid sessions: 27832\n"
     ]
    }
   ],
   "source": [
    "# Load the processed data\n",
    "# In a real scenario, we would split by time before this step.\n",
    "# For this example, we'll just load the single file and split it manually or use a subset.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "processed_path = \"data/processed_sessions\"\n",
    "schema = workflow.output_schema\n",
    "\n",
    "# Check the schema\n",
    "print(\"Schema:\", schema)\n",
    "\n",
    "# Create a Dataset\n",
    "# We can use the Merlin Dataset API\n",
    "import merlin.io\n",
    "ds = merlin.io.Dataset(processed_path, engine=\"parquet\")\n",
    "\n",
    "# Simple Time-based Split (approximate for this example)\n",
    "# We'll just take the last 20% of rows as validation since we sorted by time implicitly? \n",
    "# Actually, we should sort the parquet file or split it properly.\n",
    "# Let's assume random split for the mechanics of this demo if time split is hard to do on the fly.\n",
    "# But for session rec, time split is crucial.\n",
    "\n",
    "# Let's reload as DataFrame to split, then save back to parquet for T4Rec (easiest for small data)\n",
    "# We use glob to explicitly select .parquet files and avoid reading metadata files (like schema.pbtxt)\n",
    "parquet_files = glob.glob(os.path.join(processed_path, \"*.parquet\"))\n",
    "df = pd.read_parquet(parquet_files)\n",
    "df = df.sort_values('date_time-first')\n",
    "\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:split_index]\n",
    "valid_df = df.iloc[split_index:]\n",
    "\n",
    "train_df.to_parquet(\"data/train.parquet\")\n",
    "valid_df.to_parquet(\"data/valid.parquet\")\n",
    "\n",
    "print(f\"Train sessions: {len(train_df)}\")\n",
    "print(f\"Valid sessions: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5065a2a",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "We define the Transformer model.\n",
    "We use `SequentialBlock` to combine:\n",
    "1.  **Embeddings**: For items and side info (city, source).\n",
    "2.  **Transformer Body**: XLNet or similar.\n",
    "3.  **Prediction Head**: To predict the next item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema (Fixed): [{'name': 'item_id-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ID: 'id'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.item_id.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 85.0, 'cardinality': 1200.0}, 'domain': {'min': 0, 'max': 1199, 'name': 'item_id'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'traffic_source-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.traffic_source.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 16.0, 'cardinality': 14.0}, 'domain': {'min': 0, 'max': 13, 'name': 'traffic_source'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}, {'name': 'region_city-list', 'tags': {<Tags.CATEGORICAL: 'categorical'>}, 'properties': {'freq_threshold': 0.0, 'num_buckets': None, 'cat_path': './/categories/unique.region_city.parquet', 'max_size': 0.0, 'embedding_sizes': {'dimension': 87.0, 'cardinality': 1254.0}, 'domain': {'min': 0, 'max': 1253, 'name': 'region_city'}, 'value_count': {'min': 0, 'max': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=20)))), 'is_list': True, 'is_ragged': True}]\n",
      "Model built successfully!\n",
      "Model(\n",
      "  (heads): ModuleList(\n",
      "    (0): Head(\n",
      "      (body): SequentialBlock(\n",
      "        (0): TabularSequenceFeatures(\n",
      "          (_aggregation): ConcatFeatures()\n",
      "          (to_merge): ModuleDict(\n",
      "            (categorical_module): SequenceEmbeddingFeatures(\n",
      "              (filter_features): FilterFeatures()\n",
      "              (embedding_tables): ModuleDict(\n",
      "                (item_id-list): Embedding(1200, 64, padding_idx=0)\n",
      "                (traffic_source-list): Embedding(14, 64, padding_idx=0)\n",
      "                (region_city-list): Embedding(1254, 64, padding_idx=0)\n",
      "              )\n",
      "            )\n",
      "            (pretrained_embedding_module): PretrainedEmbeddingFeatures(\n",
      "              (filter_features): FilterFeatures()\n",
      "            )\n",
      "          )\n",
      "          (projection_module): SequentialBlock(\n",
      "            (0): DenseBlock(\n",
      "              (0): Linear(in_features=192, out_features=64, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (_masking): CausalLanguageModeling()\n",
      "        )\n",
      "        (1): TansformerBlock(\n",
      "          (transformer): XLNetModel(\n",
      "            (word_embedding): Embedding(1, 64)\n",
      "            (layer): ModuleList(\n",
      "              (0-1): 2 x XLNetLayer(\n",
      "                (rel_attn): XLNetRelativeAttention(\n",
      "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.3, inplace=False)\n",
      "                )\n",
      "                (ff): XLNetFeedForward(\n",
      "                  (layer_norm): LayerNorm((64,), eps=0.03, elementwise_affine=True)\n",
      "                  (layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                  (layer_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  (dropout): Dropout(p=0.3, inplace=False)\n",
      "                  (activation_function): GELUActivation()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.3, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (dropout): Dropout(p=0.3, inplace=False)\n",
      "          )\n",
      "          (masking): CausalLanguageModeling()\n",
      "        )\n",
      "      )\n",
      "      (prediction_task_dict): ModuleDict(\n",
      "        (next-item): NextItemPredictionTask(\n",
      "          (sequence_summary): SequenceSummary(\n",
      "            (summary): Identity()\n",
      "            (activation): Identity()\n",
      "            (first_dropout): Identity()\n",
      "            (last_dropout): Identity()\n",
      "          )\n",
      "          (metrics): ModuleList(\n",
      "            (0): RecallAt()\n",
      "            (1): NDCGAt()\n",
      "          )\n",
      "          (loss): CrossEntropyLoss()\n",
      "          (embeddings): SequenceEmbeddingFeatures(\n",
      "            (filter_features): FilterFeatures()\n",
      "            (embedding_tables): ModuleDict(\n",
      "              (item_id-list): Embedding(1200, 64, padding_idx=0)\n",
      "              (traffic_source-list): Embedding(14, 64, padding_idx=0)\n",
      "              (region_city-list): Embedding(1254, 64, padding_idx=0)\n",
      "            )\n",
      "          )\n",
      "          (item_embedding_table): Embedding(1200, 64, padding_idx=0)\n",
      "          (masking): CausalLanguageModeling()\n",
      "          (pre): Block(\n",
      "            (module): NextItemPredictionTask(\n",
      "              (item_embedding_table): Embedding(1200, 64, padding_idx=0)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the Schema for the model\n",
    "from merlin.schema import Schema, Tags\n",
    "import merlin.io\n",
    "\n",
    "# Load schema from processed data\n",
    "train_schema = merlin.io.Dataset(\"data/processed_sessions\", engine=\"parquet\").schema\n",
    "\n",
    "# Select features\n",
    "# We include all the new features we engineered\n",
    "selected_features = [\n",
    "    'item_id-list', \n",
    "    'traffic_source-list', 'region_city-list', # Original Context\n",
    "    'brand-list', 'main_category-list', 'price_bucket-list', # Item Metadata\n",
    "    'hour-list', 'day_of_week-list', 'is_weekend-list', # Temporal\n",
    "    'item_popularity-list', 'category_popularity-list' # Continuous Counters\n",
    "]\n",
    "\n",
    "input_schema = train_schema.select_by_name(selected_features)\n",
    "\n",
    "# WORKAROUND: Fix schema value_counts\n",
    "new_cols = []\n",
    "for col in input_schema:\n",
    "    props = col.properties.copy()\n",
    "    if 'value_count' not in props:\n",
    "        props['value_count'] = {}\n",
    "    props['value_count']['max'] = 20\n",
    "    new_col = col.with_properties(props)\n",
    "    new_cols.append(new_col)\n",
    "\n",
    "input_schema = Schema(new_cols)\n",
    "print(\"Input Schema (Fixed):\", input_schema)\n",
    "\n",
    "# Model parameters\n",
    "d_model = 64\n",
    "max_seq_length = 20\n",
    "\n",
    "# Define the Input Block\n",
    "# T4Rec automatically handles:\n",
    "# - Embedding tables for categorical features (brand, city, etc.)\n",
    "# - Projection for continuous features (popularity)\n",
    "# - Concatenation of all features\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    input_schema,\n",
    "    max_sequence_length=max_seq_length,\n",
    "    aggregation=\"concat\",\n",
    "    d_output=d_model,\n",
    "    masking=\"causal\",\n",
    ")\n",
    "\n",
    "# Define the Transformer Body - XLNet\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model,\n",
    "    n_head=4,\n",
    "    n_layer=2,\n",
    "    total_seq_length=max_seq_length\n",
    ")\n",
    "\n",
    "# Define body\n",
    "body = tr.SequentialBlock(\n",
    "    input_module,\n",
    "    tr.TransformerBlock(transformer_config, masking=input_module.masking)\n",
    ")\n",
    "\n",
    "# Define ranking metrics\n",
    "from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n",
    "metrics = [\n",
    "    RecallAt(top_ks=[6, 10], labels_onehot=True),\n",
    "    NDCGAt(top_ks=[6, 10], labels_onehot=True)\n",
    "]\n",
    "\n",
    "# Define the Head\n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, metrics=metrics),\n",
    "    inputs=input_module,\n",
    ")\n",
    "\n",
    "# Get the end-to-end Model\n",
    "model = tr.Model(head)\n",
    "\n",
    "print(\"Model built successfully!\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87cfb4",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We use the `Trainer` class (based on HuggingFace Trainer) to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4ba147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f9969c709843e88679fef9bf238054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5266, 'learning_rate': 0.0009000000000000001, 'epoch': 0.03}\n",
      "{'loss': 5.9934, 'learning_rate': 0.0008, 'epoch': 0.06}\n",
      "{'loss': 5.9934, 'learning_rate': 0.0008, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149fa995904449f4b70d38a4b26fd112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 7.164107322692871, 'eval_runtime': 3.5477, 'eval_samples_per_second': 7847.357, 'eval_steps_per_second': 122.615, 'epoch': 0.06}\n",
      "{'loss': 5.7634, 'learning_rate': 0.0007, 'epoch': 0.09}\n",
      "{'loss': 5.7634, 'learning_rate': 0.0007, 'epoch': 0.09}\n",
      "{'loss': 5.5622, 'learning_rate': 0.0006, 'epoch': 0.11}\n",
      "{'loss': 5.5622, 'learning_rate': 0.0006, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50430f192e034d80a587a574c3bb9e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 6.12920618057251, 'eval_runtime': 3.3524, 'eval_samples_per_second': 8304.605, 'eval_steps_per_second': 129.759, 'epoch': 0.11}\n",
      "{'loss': 5.4245, 'learning_rate': 0.0005, 'epoch': 0.14}\n",
      "{'loss': 5.4245, 'learning_rate': 0.0005, 'epoch': 0.14}\n",
      "{'loss': 5.2823, 'learning_rate': 0.0004, 'epoch': 0.17}\n",
      "{'loss': 5.2823, 'learning_rate': 0.0004, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39e41c614984e4288c03d237e692618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.769600868225098, 'eval_runtime': 3.3671, 'eval_samples_per_second': 8268.13, 'eval_steps_per_second': 129.19, 'epoch': 0.17}\n",
      "{'loss': 5.138, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 5.138, 'learning_rate': 0.0003, 'epoch': 0.2}\n",
      "{'loss': 5.1025, 'learning_rate': 0.0002, 'epoch': 0.23}\n",
      "{'loss': 5.1025, 'learning_rate': 0.0002, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c3bd90626438b8aad56a478ac2fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.535675525665283, 'eval_runtime': 3.4422, 'eval_samples_per_second': 8087.771, 'eval_steps_per_second': 126.371, 'epoch': 0.23}\n",
      "{'loss': 5.0881, 'learning_rate': 0.0001, 'epoch': 0.26}\n",
      "{'loss': 5.0881, 'learning_rate': 0.0001, 'epoch': 0.26}\n",
      "{'loss': 4.988, 'learning_rate': 0.0, 'epoch': 0.29}\n",
      "{'loss': 4.988, 'learning_rate': 0.0, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e72766a5484d99b94833708b6644cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_/loss': 5.49398946762085, 'eval_runtime': 3.5249, 'eval_samples_per_second': 7898.023, 'eval_steps_per_second': 123.407, 'epoch': 0.29}\n",
      "{'train_runtime': 33.3216, 'train_samples_per_second': 960.338, 'train_steps_per_second': 15.005, 'train_loss': 5.486896209716797, 'epoch': 0.29}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=5.486896209716797, metrics={'train_runtime': 33.3216, 'train_samples_per_second': 960.338, 'train_steps_per_second': 15.005, 'total_flos': 0.0, 'train_loss': 5.486896209716797})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers4rec.torch.trainer import Trainer\n",
    "from transformers4rec.torch.utils.data_utils import MerlinDataLoader\n",
    "\n",
    "# Training Arguments\n",
    "# Note: T4RecTrainingArguments is the correct class name in newer versions\n",
    "training_args = tr.T4RecTrainingArguments(\n",
    "    output_dir=\"./t4r_output\",\n",
    "    max_steps=500,\n",
    "    learning_rate=0.001,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    report_to=[], # Disable wandb/mlflow for now\n",
    "    dataloader_drop_last=False,\n",
    "    compute_metrics_each_n_steps=1,\n",
    "    use_mps_device=False, # Force CPU to avoid MPS errors on macOS\n",
    "    no_cuda=True # Ensure we use CPU\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=train_schema,\n",
    "    train_dataset_or_path=\"data/train.parquet\",\n",
    "    eval_dataset_or_path=\"data/valid.parquet\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcef1f",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We evaluate the model using ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7787b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ef47c0bcc8465c9d75e2fb47ced6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Evaluation Metrics:\n",
      "eval_/next-item/recall_at_6: 0.28340524435043335\n",
      "eval_/next-item/recall_at_10: 0.325452983379364\n",
      "eval_/next-item/ndcg_at_6: 0.20350845158100128\n",
      "eval_/next-item/ndcg_at_10: 0.21658070385456085\n",
      "eval_/loss: 5.49398946762085\n",
      "eval_runtime: 4.7652\n",
      "eval_samples_per_second: 5842.301\n",
      "eval_steps_per_second: 91.286\n",
      "\n",
      "Filtered Results:\n",
      "eval_/next-item/recall_at_6: 0.2834\n",
      "eval_/next-item/recall_at_10: 0.3255\n",
      "eval_/next-item/ndcg_at_6: 0.2035\n",
      "eval_/next-item/ndcg_at_10: 0.2166\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Enable metric computation (it defaults to None in Trainer)\n",
    "trainer.compute_metrics = True\n",
    "\n",
    "# Note: The trainer already has the eval_dataset_or_path from initialization\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nAll Evaluation Metrics:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nFiltered Results:\")\n",
    "for key, value in eval_metrics.items():\n",
    "    if \"recall\" in key or \"ndcg\" in key:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40bd7f",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a9b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Processed test interactions: 6434\n",
      "Processed test interactions: 6434\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Preprocess Test Data (Pandas)\n",
    "print(\"Loading test data...\")\n",
    "test_hits_df = pd.read_csv('data/metrika_hits_test.csv', low_memory=False)\n",
    "test_visits_df = pd.read_csv('data/metrika_visits_test.csv', low_memory=False)\n",
    "\n",
    "# Parse watch_ids\n",
    "test_visits_df['watch_ids_list'] = test_visits_df['watch_ids'].apply(parse_watch_ids)\n",
    "test_session_hits = test_visits_df.explode('watch_ids_list').rename(columns={'watch_ids_list': 'watch_id'})\n",
    "\n",
    "# Ensure types match\n",
    "test_session_hits['watch_id'] = test_session_hits['watch_id'].astype(str)\n",
    "test_hits_df['watch_id'] = test_hits_df['watch_id'].astype(str)\n",
    "\n",
    "# Merge\n",
    "test_full_data = test_session_hits.merge(test_hits_df, on='watch_id', how='inner')\n",
    "\n",
    "# Filter PRODUCT\n",
    "test_interactions = test_full_data[test_full_data['page_type'] == 'PRODUCT'].copy()\n",
    "\n",
    "# Sort\n",
    "test_interactions['date_time'] = pd.to_datetime(test_interactions['date_time_x'])\n",
    "test_interactions = test_interactions.sort_values(['visit_id', 'date_time'])\n",
    "\n",
    "# Rename slug -> item_id\n",
    "test_interactions = test_interactions.rename(columns={'slug': 'item_id'})\n",
    "test_interactions = test_interactions.dropna(subset=['item_id'])\n",
    "\n",
    "# Handle duplicate columns\n",
    "for col in test_interactions.columns:\n",
    "    if col.endswith('_x'):\n",
    "        base_name = col[:-2]\n",
    "        if base_name not in test_interactions.columns:\n",
    "            test_interactions = test_interactions.rename(columns={col: base_name})\n",
    "\n",
    "# Merge Product Metadata (Same as training)\n",
    "# We assume 'products_meta' is available or we reload it\n",
    "# Let's reload to be safe and self-contained\n",
    "print(\"Loading product metadata for test set...\")\n",
    "new_products = pd.read_csv('data/new_site_products.csv')\n",
    "old_products = pd.read_csv('data/old_site_products.csv')\n",
    "cols_new = ['slug', 'brand', 'main_category', 'price_per_period_week']\n",
    "cols_old = ['slug', 'brand', 'main_category', 'price_per_period_week']\n",
    "products_combined = pd.concat([new_products[cols_new], old_products[cols_old]])\n",
    "products_meta = products_combined.drop_duplicates(subset=['slug']).copy()\n",
    "products_meta['brand'] = products_meta['brand'].fillna('Unknown')\n",
    "products_meta['main_category'] = products_meta['main_category'].fillna('Unknown')\n",
    "products_meta['price_per_period_week'] = products_meta['price_per_period_week'].fillna(0)\n",
    "\n",
    "test_interactions = test_interactions.merge(products_meta, left_on='item_id', right_on='slug', how='left')\n",
    "test_interactions['brand'] = test_interactions['brand'].fillna('Unknown')\n",
    "test_interactions['main_category'] = test_interactions['main_category'].fillna('Unknown')\n",
    "test_interactions['price_per_period_week'] = test_interactions['price_per_period_week'].fillna(0)\n",
    "\n",
    "# Feature Engineering (Same as training)\n",
    "test_interactions['hour'] = test_interactions['date_time'].dt.hour\n",
    "test_interactions['day_of_week'] = test_interactions['date_time'].dt.dayofweek\n",
    "test_interactions['is_weekend'] = test_interactions['day_of_week'].isin([5, 6]).astype(int)\n",
    "test_interactions['price_bucket'] = pd.qcut(test_interactions['price_per_period_week'], q=10, labels=False, duplicates='drop').fillna(0).astype(int)\n",
    "\n",
    "# Select columns\n",
    "cols_to_keep = [\n",
    "    'visit_id', 'item_id', 'date_time', \n",
    "    'traffic_source', 'region_city',\n",
    "    'brand', 'main_category', 'price_bucket',\n",
    "    'hour', 'day_of_week', 'is_weekend'\n",
    "]\n",
    "test_interactions = test_interactions[cols_to_keep]\n",
    "\n",
    "print(f\"Processed test interactions: {len(test_interactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading category maps...\n",
      "Mapping columns...\n",
      "Grouping by session...\n",
      "Processed 2062 test sessions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>item_id-list</th>\n",
       "      <th>traffic_source-list</th>\n",
       "      <th>region_city-list</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3705189088312688779</td>\n",
       "      <td>[601]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>2025-07-02 17:09:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3705579516103753793</td>\n",
       "      <td>[294, 497, 519, 519]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>2025-07-02 17:34:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3706260855703732415</td>\n",
       "      <td>[775, 775]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2025-07-02 18:17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3706380385759789156</td>\n",
       "      <td>[373, 373]</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2025-07-02 18:25:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3706985600653983919</td>\n",
       "      <td>[705, 705]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2025-07-02 19:03:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              visit_id          item_id-list traffic_source-list  \\\n",
       "0  3705189088312688779                 [601]                 [4]   \n",
       "1  3705579516103753793  [294, 497, 519, 519]        [1, 1, 1, 1]   \n",
       "2  3706260855703732415            [775, 775]              [4, 4]   \n",
       "3  3706380385759789156            [373, 373]              [5, 5]   \n",
       "4  3706985600653983919            [705, 705]              [4, 4]   \n",
       "\n",
       "  region_city-list           date_time  \n",
       "0              [1] 2025-07-02 17:09:34  \n",
       "1     [0, 0, 0, 0] 2025-07-02 17:34:23  \n",
       "2           [1, 1] 2025-07-02 18:17:43  \n",
       "3           [1, 1] 2025-07-02 18:25:19  \n",
       "4           [1, 1] 2025-07-02 19:03:47  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Manual Transformation (Pandas)\n",
    "# We map the categorical features using the dictionaries created during training\n",
    "\n",
    "def load_map(col_name):\n",
    "    path = f\"categories/unique.{col_name}.parquet\"\n",
    "    # Check if file exists (some might not be created if cardinality is low or handled differently)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: Map for {col_name} not found at {path}\")\n",
    "        return {}\n",
    "        \n",
    "    df = pd.read_parquet(path)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return {val: i + 1 for i, val in enumerate(df[col_name])}\n",
    "\n",
    "print(\"Loading category maps...\")\n",
    "# Load maps for ALL categorical features\n",
    "cat_feats = ['item_id', 'traffic_source', 'region_city', 'brand', 'main_category', \n",
    "             'price_bucket', 'hour', 'day_of_week', 'is_weekend']\n",
    "maps = {col: load_map(col) for col in cat_feats}\n",
    "\n",
    "# Apply mappings\n",
    "print(\"Mapping categorical columns...\")\n",
    "for col in cat_feats:\n",
    "    # Map and fillna(0) for unknown/padding\n",
    "    test_interactions[f'{col}_mapped'] = test_interactions[col].map(maps[col]).fillna(0).astype(int)\n",
    "\n",
    "# Handle Continuous Features (Popularity)\n",
    "# We need to use the TRAINING data statistics to ensure consistency\n",
    "print(\"Calculating continuous features based on training stats...\")\n",
    "train_data = pd.read_parquet('data/enriched_interactions.parquet')\n",
    "\n",
    "# 1. Compute Counts (Popularity)\n",
    "item_counts = train_data['item_id'].value_counts()\n",
    "category_counts = train_data['main_category'].value_counts()\n",
    "\n",
    "test_interactions['item_popularity'] = test_interactions['item_id'].map(item_counts).fillna(0)\n",
    "test_interactions['category_popularity'] = test_interactions['main_category'].map(category_counts).fillna(0)\n",
    "\n",
    "# 2. LogOp (np.log(x + 1))\n",
    "test_interactions['item_popularity'] = np.log(test_interactions['item_popularity'] + 1)\n",
    "test_interactions['category_popularity'] = np.log(test_interactions['category_popularity'] + 1)\n",
    "\n",
    "# 3. Normalize ((x - mean) / std)\n",
    "# Compute stats from TRAINING data (after LogOp)\n",
    "train_item_pop = np.log(train_data['item_popularity'] + 1)\n",
    "train_cat_pop = np.log(train_data['category_popularity'] + 1)\n",
    "\n",
    "test_interactions['item_popularity'] = (test_interactions['item_popularity'] - train_item_pop.mean()) / train_item_pop.std()\n",
    "test_interactions['category_popularity'] = (test_interactions['category_popularity'] - train_cat_pop.mean()) / train_cat_pop.std()\n",
    "\n",
    "# Groupby to create lists\n",
    "print(\"Grouping by session...\")\n",
    "agg_dict = {f'{col}_mapped': list for col in cat_feats}\n",
    "agg_dict['item_popularity'] = list\n",
    "agg_dict['category_popularity'] = list\n",
    "agg_dict['date_time'] = 'first'\n",
    "\n",
    "test_grouped = test_interactions.groupby('visit_id').agg(agg_dict).reset_index()\n",
    "\n",
    "# Rename columns to match schema expected by T4Rec\n",
    "rename_dict = {f'{col}_mapped': f'{col}-list' for col in cat_feats}\n",
    "rename_dict['item_popularity'] = 'item_popularity-list'\n",
    "rename_dict['category_popularity'] = 'category_popularity-list'\n",
    "\n",
    "test_grouped = test_grouped.rename(columns=rename_dict)\n",
    "\n",
    "# Save to parquet\n",
    "test_grouped.to_parquet(\"data/processed_test_sessions_pandas.parquet\")\n",
    "print(f\"Processed {len(test_grouped)} test sessions.\")\n",
    "test_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "557fbcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/t4rec_env/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38d52f862854b6c9cdc9e862cae928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (2062, 6)\n"
     ]
    }
   ],
   "source": [
    "# 3. Generate Predictions\n",
    "\n",
    "# Update args to return top-6 predictions\n",
    "training_args.predict_top_k = 6\n",
    "trainer.args = training_args\n",
    "\n",
    "# Predict\n",
    "print(\"Generating predictions...\")\n",
    "# Note: Trainer.predict() in T4Rec expects a dataset object or uses the one from init\n",
    "# We need to manually create a dataloader or dataset for the test set\n",
    "import merlin.io\n",
    "test_dataset = merlin.io.Dataset(\"data/processed_test_sessions_pandas.parquet\", engine=\"parquet\")\n",
    "\n",
    "# We can pass the dataset directly to predict\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# The predictions object contains a tuple (item_ids, scores) because we set predict_top_k\n",
    "# We extract the item IDs (first element of the tuple)\n",
    "# Note: Depending on T4Rec version, it might be in predictions.predictions or just predictions\n",
    "if isinstance(test_predictions.predictions, tuple):\n",
    "    top_k_ids = test_predictions.predictions[0]\n",
    "else:\n",
    "    top_k_ids = test_predictions.predictions\n",
    "\n",
    "print(\"Predictions shape:\", top_k_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a7e39fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product catalogs...\n",
      "Loaded 1400 product mappings.\n",
      "Calculating popular items for fallback...\n",
      "Found 50 popular product IDs.\n",
      "Default prediction: 3714 3282 5602 3582 3365 3746\n",
      "Mapping predictions...\n",
      "Generating final submission file...\n",
      "Submission saved to submission.csv with 3891 rows.\n",
      "              visit_id                                   product_ids\n",
      "0  3705073560174199024                 3714 3282 5602 3582 3365 3746\n",
      "1  3705189088312688779  3942 463480486 463480210 3631 3653 495264803\n",
      "2  3705549051029618879                 3714 3282 5602 3582 3365 3746\n",
      "3  3705579516103753793            3393 3625 463480227 3942 3405 3301\n",
      "4  3705717843210797336                 3714 3282 5602 3582 3365 3746\n",
      "Found 50 popular product IDs.\n",
      "Default prediction: 3714 3282 5602 3582 3365 3746\n",
      "Mapping predictions...\n",
      "Generating final submission file...\n",
      "Submission saved to submission.csv with 3891 rows.\n",
      "              visit_id                                   product_ids\n",
      "0  3705073560174199024                 3714 3282 5602 3582 3365 3746\n",
      "1  3705189088312688779  3942 463480486 463480210 3631 3653 495264803\n",
      "2  3705549051029618879                 3714 3282 5602 3582 3365 3746\n",
      "3  3705579516103753793            3393 3625 463480227 3942 3405 3301\n",
      "4  3705717843210797336                 3714 3282 5602 3582 3365 3746\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Submission File\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Product Mappings (Slug -> Numeric ID)\n",
    "print(\"Loading product catalogs...\")\n",
    "try:\n",
    "    new_products = pd.read_csv('data/new_site_products.csv', usecols=['id', 'slug'])\n",
    "    old_products = pd.read_csv('data/old_site_products.csv', usecols=['id', 'slug'])\n",
    "    \n",
    "    # Combine and create map\n",
    "    products_df = pd.concat([new_products, old_products]).drop_duplicates(subset=['slug'])\n",
    "    # Ensure IDs are strings\n",
    "    products_df['id'] = products_df['id'].astype(str)\n",
    "    slug_to_id = dict(zip(products_df['slug'], products_df['id']))\n",
    "    \n",
    "    # Create a list of all available IDs for absolute fallback\n",
    "    all_product_ids = products_df['id'].tolist()\n",
    "    \n",
    "    print(f\"Loaded {len(slug_to_id)} product mappings.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading product catalogs: {e}\")\n",
    "    slug_to_id = {}\n",
    "    all_product_ids = []\n",
    "\n",
    "# 2. Determine Popular Items (Fallback for cold users)\n",
    "print(\"Calculating popular items for fallback...\")\n",
    "try:\n",
    "    # IMPORTANT: Filter for page_type='PRODUCT' to avoid counting 'MAIN', 'CATALOG' etc.\n",
    "    hits_df = pd.read_csv('data/metrika_hits.csv', usecols=['slug', 'page_type'])\n",
    "    product_hits = hits_df[hits_df['page_type'] == 'PRODUCT']\n",
    "    \n",
    "    # Get top 50 to be safe\n",
    "    top_slugs = product_hits['slug'].value_counts().head(50).index.tolist()\n",
    "    \n",
    "    # Map to IDs\n",
    "    top_ids = []\n",
    "    for s in top_slugs:\n",
    "        if s in slug_to_id:\n",
    "            tid = str(slug_to_id[s])\n",
    "            if tid not in top_ids:\n",
    "                top_ids.append(tid)\n",
    "    \n",
    "    print(f\"Found {len(top_ids)} popular product IDs.\")\n",
    "    \n",
    "    # Ensure we have at least 6\n",
    "    if len(top_ids) < 6:\n",
    "        print(\"Warning: Not enough popular items found. Padding with random products.\")\n",
    "        # Pad with any available products\n",
    "        for pid in all_product_ids:\n",
    "            if pid not in top_ids:\n",
    "                top_ids.append(pid)\n",
    "            if len(top_ids) >= 6:\n",
    "                break\n",
    "        \n",
    "    default_pred_str = \" \".join(top_ids[:6])\n",
    "    print(f\"Default prediction: {default_pred_str}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating popular items: {e}\")\n",
    "    # Absolute fallback\n",
    "    if len(all_product_ids) >= 6:\n",
    "        default_pred_str = \" \".join(all_product_ids[:6])\n",
    "    else:\n",
    "        default_pred_str = \"0 0 0 0 0 0\" # Should not happen if data is loaded\n",
    "\n",
    "# 3. Map Model Predictions to Product IDs\n",
    "# reverse_item_map: Model Int -> Slug\n",
    "reverse_item_map = {i + 1: val for i, val in enumerate(pd.read_parquet(\"categories/unique.item_id.parquet\")['item_id'])}\n",
    "reverse_item_map[0] = \"unknown\"\n",
    "\n",
    "# Get visit_ids from the processed test set\n",
    "processed_test = pd.read_parquet(\"data/processed_test_sessions_pandas.parquet\")\n",
    "predicted_visit_ids = processed_test['visit_id'].astype(str).values\n",
    "\n",
    "prediction_map = {}\n",
    "print(\"Mapping predictions...\")\n",
    "for i, vid in enumerate(predicted_visit_ids):\n",
    "    model_ids = top_k_ids[i] # Array of model integers\n",
    "    real_ids = []\n",
    "    for mid in model_ids:\n",
    "        slug = reverse_item_map.get(mid, \"unknown\")\n",
    "        if slug in slug_to_id:\n",
    "            real_ids.append(str(slug_to_id[slug]))\n",
    "    \n",
    "    # If we didn't find 6 valid IDs, pad with popular items\n",
    "    if len(real_ids) < 6:\n",
    "        # Only add unique popular items that aren't already predicted\n",
    "        for pid in top_ids:\n",
    "            if pid not in real_ids:\n",
    "                real_ids.append(pid)\n",
    "            if len(real_ids) >= 6:\n",
    "                break\n",
    "        \n",
    "    prediction_map[vid] = \" \".join(real_ids[:6])\n",
    "\n",
    "# 4. Generate Final Submission for ALL Test Visits\n",
    "print(\"Generating final submission file...\")\n",
    "test_visits = pd.read_csv('data/metrika_visits_test.csv', usecols=['visit_id'])\n",
    "test_visits['visit_id'] = test_visits['visit_id'].astype(str)\n",
    "\n",
    "def get_pred(vid):\n",
    "    return prediction_map.get(vid, default_pred_str)\n",
    "\n",
    "test_visits['product_ids'] = test_visits['visit_id'].apply(get_pred)\n",
    "\n",
    "# Save\n",
    "test_visits.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"Submission saved to submission.csv with {len(test_visits)} rows.\")\n",
    "print(test_visits.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t4rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
