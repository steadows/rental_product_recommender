{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4871fd0f",
   "metadata": {},
   "source": [
    "# Feature Engineering for Rental Product Recommender\n",
    "\n",
    "This notebook implements advanced feature engineering techniques to enrich the dataset for the Transformers4Rec model.\n",
    "Inspired by \"Feature Engineering for Recommendation Systems\", we will focus on:\n",
    "1.  **Item Metadata**: Enriching item representations with Brand, Category, and Price.\n",
    "2.  **Session Context**: Extracting temporal and device-specific features.\n",
    "3.  **Counter Features**: Calculating global popularity metrics.\n",
    "\n",
    "## 1. Load Data\n",
    "We load the raw interaction logs and the product catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4b500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Loading product catalogs...\n",
      "Hits: 1721596, Visits: 323241\n",
      "New Products: 665, Old Products: 761\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Load Raw Data\n",
    "print(\"Loading raw data...\")\n",
    "hits_df = pd.read_csv('data/metrika_hits.csv', low_memory=False)\n",
    "visits_df = pd.read_csv('data/metrika_visits.csv', low_memory=False)\n",
    "\n",
    "# Load Product Catalogs\n",
    "print(\"Loading product catalogs...\")\n",
    "new_products = pd.read_csv('data/new_site_products.csv')\n",
    "old_products = pd.read_csv('data/old_site_products.csv')\n",
    "\n",
    "print(f\"Hits: {len(hits_df)}, Visits: {len(visits_df)}\")\n",
    "print(f\"New Products: {len(new_products)}, Old Products: {len(old_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929072bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Products with Metadata: 1400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_category</th>\n",
       "      <th>price_per_period_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>matras-red-castle-kokon-dlya-novorozhdennyh-co...</td>\n",
       "      <td>Red Castle</td>\n",
       "      <td>Коконы для новорожденных</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kokon-dlya-novorozhdennyh-matello-cocon-baby-l...</td>\n",
       "      <td>Matello</td>\n",
       "      <td>Коконы для новорожденных</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kokon-lyulka-dlya-novorozhdennyh-farla-baby-sh...</td>\n",
       "      <td>Farla</td>\n",
       "      <td>Коконы для новорожденных</td>\n",
       "      <td>1300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kacheli-shezlong-4moms-mamaroo-40-naprokat</td>\n",
       "      <td>4moms</td>\n",
       "      <td>Электрокачели</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kacheli-shezlong-4moms-mamaroo-30-naprokat</td>\n",
       "      <td>4moms</td>\n",
       "      <td>Электрокачели</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                slug       brand  \\\n",
       "0  matras-red-castle-kokon-dlya-novorozhdennyh-co...  Red Castle   \n",
       "1  kokon-dlya-novorozhdennyh-matello-cocon-baby-l...     Matello   \n",
       "2  kokon-lyulka-dlya-novorozhdennyh-farla-baby-sh...       Farla   \n",
       "3         kacheli-shezlong-4moms-mamaroo-40-naprokat       4moms   \n",
       "4         kacheli-shezlong-4moms-mamaroo-30-naprokat       4moms   \n",
       "\n",
       "              main_category  price_per_period_week  \n",
       "0  Коконы для новорожденных                 1500.0  \n",
       "1  Коконы для новорожденных                 1500.0  \n",
       "2  Коконы для новорожденных                 1300.0  \n",
       "3             Электрокачели                 2200.0  \n",
       "4             Электрокачели                 2700.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Unify Product Metadata\n",
    "# We need to map 'slug' (used in interactions) to metadata like Brand, Category, Price.\n",
    "\n",
    "# Select relevant columns and unify names\n",
    "cols_new = ['slug', 'brand', 'main_category', 'price_per_period_week']\n",
    "cols_old = ['slug', 'brand', 'main_category', 'price_per_period_week']\n",
    "\n",
    "# Normalize column names if they differ (checking file headers from previous steps)\n",
    "# new_site_products.csv has 'main_category', 'price_per_period_week'\n",
    "# old_site_products.csv has 'main_category', 'price_per_period_week'\n",
    "# It seems they match based on previous `head` output.\n",
    "\n",
    "products_combined = pd.concat([\n",
    "    new_products[cols_new],\n",
    "    old_products[cols_old]\n",
    "])\n",
    "\n",
    "# Drop duplicates (same slug might appear in both or multiple times)\n",
    "# We keep the first occurrence (arbitrary, but usually fine for static metadata)\n",
    "products_meta = products_combined.drop_duplicates(subset=['slug']).copy()\n",
    "\n",
    "# Fill missing values\n",
    "products_meta['brand'] = products_meta['brand'].fillna('Unknown')\n",
    "products_meta['main_category'] = products_meta['main_category'].fillna('Unknown')\n",
    "products_meta['price_per_period_week'] = products_meta['price_per_period_week'].fillna(0)\n",
    "\n",
    "print(f\"Unique Products with Metadata: {len(products_meta)}\")\n",
    "products_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f9c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions with Metadata: 408562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_category</th>\n",
       "      <th>price_per_period_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463311640199432</td>\n",
       "      <td>avtokreslo-chicco-synthesis-xt-plus</td>\n",
       "      <td>Chicсo</td>\n",
       "      <td>Автокресла</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640428033179772</td>\n",
       "      <td>manezh-krovat-capella-best-friends</td>\n",
       "      <td>Capella</td>\n",
       "      <td>Манежи и кроватки</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>ELC</td>\n",
       "      <td>Машинки, рули и гаражи</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>ELC</td>\n",
       "      <td>Машинки, рули и гаражи</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>714740689010850</td>\n",
       "      <td>piratskiy-korabl-elc</td>\n",
       "      <td>ELC</td>\n",
       "      <td>Машинки, рули и гаражи</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          visit_id                              item_id    brand  \\\n",
       "0  463311640199432  avtokreslo-chicco-synthesis-xt-plus   Chicсo   \n",
       "1  640428033179772   manezh-krovat-capella-best-friends  Capella   \n",
       "2  714740689010850                 piratskiy-korabl-elc      ELC   \n",
       "3  714740689010850                 piratskiy-korabl-elc      ELC   \n",
       "4  714740689010850                 piratskiy-korabl-elc      ELC   \n",
       "\n",
       "            main_category  price_per_period_week  \n",
       "0              Автокресла                 1000.0  \n",
       "1       Манежи и кроватки                 1000.0  \n",
       "2  Машинки, рули и гаражи                  800.0  \n",
       "3  Машинки, рули и гаражи                  800.0  \n",
       "4  Машинки, рули и гаражи                  800.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Reconstruct Sessions and Merge Metadata\n",
    "\n",
    "# Parse watch_ids\n",
    "def parse_watch_ids(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "visits_df['watch_ids_list'] = visits_df['watch_ids'].apply(parse_watch_ids)\n",
    "session_hits = visits_df.explode('watch_ids_list').rename(columns={'watch_ids_list': 'watch_id'})\n",
    "\n",
    "# Ensure types match\n",
    "session_hits['watch_id'] = session_hits['watch_id'].astype(str)\n",
    "hits_df['watch_id'] = hits_df['watch_id'].astype(str)\n",
    "\n",
    "# Merge Hits with Visits (Session Context)\n",
    "full_data = session_hits.merge(hits_df, on='watch_id', how='inner')\n",
    "\n",
    "# Filter for PRODUCT interactions\n",
    "interactions = full_data[full_data['page_type'] == 'PRODUCT'].copy()\n",
    "\n",
    "# Sort by time\n",
    "interactions['date_time'] = pd.to_datetime(interactions['date_time_x'])\n",
    "interactions = interactions.sort_values(['visit_id', 'date_time'])\n",
    "\n",
    "# Rename slug -> item_id for consistency\n",
    "interactions = interactions.rename(columns={'slug': 'item_id'})\n",
    "\n",
    "# Merge Product Metadata\n",
    "interactions = interactions.merge(products_meta, left_on='item_id', right_on='slug', how='left')\n",
    "\n",
    "# Fill missing metadata for items not in catalog\n",
    "interactions['brand'] = interactions['brand'].fillna('Unknown')\n",
    "interactions['main_category'] = interactions['main_category'].fillna('Unknown')\n",
    "interactions['price_per_period_week'] = interactions['price_per_period_week'].fillna(0)\n",
    "\n",
    "print(f\"Interactions with Metadata: {len(interactions)}\")\n",
    "interactions[['visit_id', 'item_id', 'brand', 'main_category', 'price_per_period_week']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5a751e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Interactions:\n",
      "          visit_id                              item_id           date_time  \\\n",
      "0  463311640199432  avtokreslo-chicco-synthesis-xt-plus 2022-01-20 03:29:26   \n",
      "1  640428033179772   manezh-krovat-capella-best-friends 2022-01-20 03:40:42   \n",
      "2  714740689010850                 piratskiy-korabl-elc 2022-01-20 03:45:26   \n",
      "3  714740689010850                 piratskiy-korabl-elc 2022-01-20 03:45:26   \n",
      "4  714740689010850                 piratskiy-korabl-elc 2022-01-20 03:45:26   \n",
      "\n",
      "  traffic_source    brand           main_category  price_bucket  hour  \\\n",
      "0             ad   Chicсo              Автокресла             3     3   \n",
      "1             ad  Capella       Манежи и кроватки             3     3   \n",
      "2         direct      ELC  Машинки, рули и гаражи             2     3   \n",
      "3         direct      ELC  Машинки, рули и гаражи             2     3   \n",
      "4         direct      ELC  Машинки, рули и гаражи             2     3   \n",
      "\n",
      "   day_of_week  is_weekend  item_popularity  category_popularity  \n",
      "0            3           0             1119                65753  \n",
      "1            3           0              406                22955  \n",
      "2            3           0              351                10707  \n",
      "3            3           0              351                10707  \n",
      "4            3           0              351                10707  \n",
      "Saved to data/enriched_interactions.parquet\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "# A. Temporal Features\n",
    "interactions['hour'] = interactions['date_time'].dt.hour\n",
    "interactions['day_of_week'] = interactions['date_time'].dt.dayofweek\n",
    "interactions['is_weekend'] = interactions['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# B. Counter Features (Global Popularity)\n",
    "# Item Popularity\n",
    "item_counts = interactions['item_id'].value_counts()\n",
    "interactions['item_popularity'] = interactions['item_id'].map(item_counts)\n",
    "\n",
    "# Category Popularity\n",
    "category_counts = interactions['main_category'].value_counts()\n",
    "interactions['category_popularity'] = interactions['main_category'].map(category_counts)\n",
    "\n",
    "# C. Price Binning (Optional, but good for categorical models)\n",
    "# We can keep price as continuous or bin it. Let's keep it continuous for now, \n",
    "# but T4Rec might handle categorical better if we don't normalize.\n",
    "# Let's create a 'price_bucket' feature.\n",
    "interactions['price_bucket'] = pd.qcut(interactions['price_per_period_week'], q=10, labels=False, duplicates='drop').fillna(0).astype(int)\n",
    "\n",
    "# Select Final Columns\n",
    "# We keep the original IDs and the new features\n",
    "final_cols = [\n",
    "    'visit_id', 'item_id', 'date_time', \n",
    "    'traffic_source', 'region_city', # Original Context\n",
    "    'brand', 'main_category', 'price_bucket', # Item Metadata\n",
    "    'hour', 'day_of_week', 'is_weekend', # Temporal\n",
    "    'device_category', 'mobile_phone', # Device\n",
    "    'item_popularity', 'category_popularity' # Counters\n",
    "]\n",
    "\n",
    "# Ensure columns exist (handle potential missing ones from merge)\n",
    "available_cols = [c for c in final_cols if c in interactions.columns]\n",
    "enriched_interactions = interactions[available_cols]\n",
    "\n",
    "print(\"Enriched Interactions:\")\n",
    "print(enriched_interactions.head())\n",
    "\n",
    "# Save to Parquet for T4Rec\n",
    "# We save this as a new \"raw\" file for the T4Rec notebook to pick up\n",
    "enriched_interactions.to_parquet('data/enriched_interactions.parquet', index=False)\n",
    "print(\"Saved to data/enriched_interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ce1af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Add-to-Cart events (this might take a moment)...\n",
      "'ecommerce' column found in interactions.\n",
      "Total Add-to-Cart events found: 1062\n",
      "Top 5 Items by Conversion Rate (min 10 views):\n",
      "                                               item_id  total_views  \\\n",
      "441  kacheli-shezlong-4moms-mamaroo-40-seryy-plyush...           24   \n",
      "66               avtokreslo-joie-bold-9-36-kg-naprokat           27   \n",
      "603  kovrik-parklon-portable-malyshariki-s-sumkoy-n...           11   \n",
      "54               avtokreslo-chicco-youniverse-naprokat          157   \n",
      "650         manezh-krovat-chicco-lullaby-race-naprokat           12   \n",
      "\n",
      "     total_adds  conversion_rate  \n",
      "441           5         0.208333  \n",
      "66            5         0.185185  \n",
      "603           2         0.181818  \n",
      "54           27         0.171975  \n",
      "650           2         0.166667  \n",
      "Saved enriched data with Conversion Rates.\n",
      "Final columns: ['visit_id', 'item_id', 'date_time', 'traffic_source', 'region_city', 'brand', 'main_category', 'price_bucket', 'hour', 'day_of_week', 'is_weekend', 'device_category', 'mobile_phone', 'item_popularity', 'category_popularity', 'conversion_rate']\n"
     ]
    }
   ],
   "source": [
    "# 5. Advanced Feature Engineering: Conversion Rates (Phase 2B)\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Function to parse ecommerce JSON and extract \"add\" events\n",
    "def extract_add_to_cart(row):\n",
    "    ecommerce_data = row['ecommerce']\n",
    "    if pd.isna(ecommerce_data) or ecommerce_data == '':\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # The data is often a string representation of a list of dicts\n",
    "        if isinstance(ecommerce_data, str):\n",
    "            try:\n",
    "                data = json.loads(ecommerce_data)\n",
    "            except:\n",
    "                try:\n",
    "                    data = ast.literal_eval(ecommerce_data)\n",
    "                except:\n",
    "                    return 0\n",
    "        else:\n",
    "            data = ecommerce_data\n",
    "            \n",
    "        # Check for list or dict\n",
    "        if isinstance(data, list):\n",
    "            for event in data:\n",
    "                if 'add' in event:\n",
    "                    return 1\n",
    "        elif isinstance(data, dict):\n",
    "            if 'add' in data:\n",
    "                return 1\n",
    "                \n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "print(\"Extracting Add-to-Cart events (this might take a moment)...\")\n",
    "\n",
    "# Check if 'ecommerce' column is already in interactions\n",
    "if 'ecommerce' in interactions.columns:\n",
    "    print(\"'ecommerce' column found in interactions.\")\n",
    "    interactions_with_ecommerce = interactions.copy()\n",
    "else:\n",
    "    print(\"'ecommerce' column NOT found. Merging from hits_df...\")\n",
    "    interactions_with_ecommerce = interactions.merge(hits_df[['watch_id', 'ecommerce']], on='watch_id', how='left')\n",
    "\n",
    "# Apply extraction\n",
    "interactions_with_ecommerce['is_add_to_cart'] = interactions_with_ecommerce.apply(extract_add_to_cart, axis=1)\n",
    "\n",
    "print(f\"Total Add-to-Cart events found: {interactions_with_ecommerce['is_add_to_cart'].sum()}\")\n",
    "\n",
    "# Calculate Conversion Rates per Item\n",
    "item_stats = interactions_with_ecommerce.groupby('item_id').agg(\n",
    "    total_views=('visit_id', 'count'),\n",
    "    total_adds=('is_add_to_cart', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "item_stats['conversion_rate'] = item_stats['total_adds'] / item_stats['total_views']\n",
    "item_stats['conversion_rate'] = item_stats['conversion_rate'].fillna(0)\n",
    "\n",
    "# Merge back to interactions\n",
    "if 'conversion_rate' in interactions.columns:\n",
    "    interactions = interactions.drop(columns=['conversion_rate'])\n",
    "\n",
    "interactions = interactions.merge(item_stats[['item_id', 'conversion_rate']], on='item_id', how='left')\n",
    "\n",
    "print(\"Top 5 Items by Conversion Rate (min 10 views):\")\n",
    "print(item_stats[item_stats['total_views'] > 10].sort_values('conversion_rate', ascending=False).head())\n",
    "\n",
    "# Fix column names (handle _x suffixes from merge)\n",
    "# We prefer the session-level data (from visits_df, which was left side _x)\n",
    "rename_map = {\n",
    "    'region_city_x': 'region_city',\n",
    "    'device_category_x': 'device_category',\n",
    "    'mobile_phone_x': 'mobile_phone'\n",
    "}\n",
    "interactions = interactions.rename(columns=rename_map)\n",
    "\n",
    "# Update final columns to include conversion_rate\n",
    "if 'conversion_rate' not in final_cols:\n",
    "    final_cols.append('conversion_rate')\n",
    "\n",
    "# Ensure all final_cols exist\n",
    "missing_cols = [c for c in final_cols if c not in interactions.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns {missing_cols}. Checking for alternatives...\")\n",
    "    # Try to find them with suffixes\n",
    "    for col in missing_cols:\n",
    "        if col + '_x' in interactions.columns:\n",
    "            interactions[col] = interactions[col + '_x']\n",
    "            print(f\"Recovered {col} from {col}_x\")\n",
    "        elif col + '_y' in interactions.columns:\n",
    "            interactions[col] = interactions[col + '_y']\n",
    "            print(f\"Recovered {col} from {col}_y\")\n",
    "\n",
    "# Re-save enriched data\n",
    "# Use intersection of available columns to avoid KeyError\n",
    "available_cols = [c for c in final_cols if c in interactions.columns]\n",
    "enriched_interactions = interactions[available_cols]\n",
    "enriched_interactions.to_parquet('data/enriched_interactions.parquet', index=False)\n",
    "print(\"Saved enriched data with Conversion Rates.\")\n",
    "print(f\"Final columns: {enriched_interactions.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9fb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating User History Features...\n",
      "User History Features Added:\n",
      "         client_id              visit_id  user_session_rank  \\\n",
      "0       1335930034   7155825714110136555                  1   \n",
      "1   16492904938604   1744225743145009250                  1   \n",
      "2   16506143675549   2091271487556157575                  1   \n",
      "3  164190539534859  18255010828911116330                  1   \n",
      "4  164482580996736    573835779580362831                  1   \n",
      "5  164482580996736    573835779580362831                  1   \n",
      "6  164482580996736    573835779580362831                  1   \n",
      "7  164482580996736    573835779580362831                  1   \n",
      "8  164482580996736    573835779580362831                  1   \n",
      "9  164482580996736    573835779580362831                  1   \n",
      "\n",
      "   days_since_last_session  is_new_user  \n",
      "0                     -1.0            1  \n",
      "1                     -1.0            1  \n",
      "2                     -1.0            1  \n",
      "3                     -1.0            1  \n",
      "4                     -1.0            1  \n",
      "5                     -1.0            1  \n",
      "6                     -1.0            1  \n",
      "7                     -1.0            1  \n",
      "8                     -1.0            1  \n",
      "9                     -1.0            1  \n",
      "Saved enriched data with User History.\n",
      "Final columns: ['visit_id', 'item_id', 'date_time', 'traffic_source', 'region_city', 'brand', 'main_category', 'price_bucket', 'hour', 'day_of_week', 'is_weekend', 'device_category', 'mobile_phone', 'item_popularity', 'category_popularity', 'conversion_rate', 'user_session_rank', 'days_since_last_session', 'is_new_user']\n"
     ]
    }
   ],
   "source": [
    "# 6. User History Features (Phase 3)\n",
    "\n",
    "print(\"Calculating User History Features...\")\n",
    "\n",
    "# Ensure client_id is available and consistent\n",
    "if 'client_id' not in interactions.columns:\n",
    "    if 'client_id_x' in interactions.columns:\n",
    "        interactions['client_id'] = interactions['client_id_x']\n",
    "    elif 'client_id_y' in interactions.columns:\n",
    "        interactions['client_id'] = interactions['client_id_y']\n",
    "\n",
    "# Sort by User and Time to ensure correct history calculation\n",
    "interactions = interactions.sort_values(['client_id', 'date_time'])\n",
    "\n",
    "# 1. Session Rank (1st session, 2nd session, etc.)\n",
    "# We group by client_id and rank the visit_ids by time\n",
    "# dense rank means 1, 2, 3... with no gaps\n",
    "interactions['user_session_rank'] = interactions.groupby('client_id')['visit_id'].transform(lambda x: x.factorize()[0] + 1)\n",
    "\n",
    "# 2. Days Since Last Session (Recency)\n",
    "# We need a dataframe of unique sessions per user with their timestamps\n",
    "user_sessions = interactions[['client_id', 'visit_id', 'date_time']].drop_duplicates(subset=['visit_id'])\n",
    "user_sessions = user_sessions.sort_values(['client_id', 'date_time'])\n",
    "\n",
    "# Calculate time difference between current and previous session\n",
    "user_sessions['prev_session_time'] = user_sessions.groupby('client_id')['date_time'].shift(1)\n",
    "user_sessions['days_since_last_session'] = (user_sessions['date_time'] - user_sessions['prev_session_time']).dt.total_seconds() / (24 * 3600)\n",
    "user_sessions['days_since_last_session'] = user_sessions['days_since_last_session'].fillna(-1) # -1 for first session\n",
    "\n",
    "# Merge recency back to interactions\n",
    "interactions = interactions.merge(user_sessions[['visit_id', 'days_since_last_session']], on='visit_id', how='left')\n",
    "\n",
    "# 3. Is New User (from raw data, double check)\n",
    "if 'is_new_user' not in interactions.columns:\n",
    "    # Try to recover from raw visits if possible, or infer from rank\n",
    "    interactions['is_new_user'] = (interactions['user_session_rank'] == 1).astype(int)\n",
    "else:\n",
    "    # Ensure it's numeric\n",
    "    interactions['is_new_user'] = interactions['is_new_user'].astype(int)\n",
    "\n",
    "print(\"User History Features Added:\")\n",
    "print(interactions[['client_id', 'visit_id', 'user_session_rank', 'days_since_last_session', 'is_new_user']].head(10))\n",
    "\n",
    "# Add to final columns\n",
    "new_history_cols = ['user_session_rank', 'days_since_last_session', 'is_new_user']\n",
    "for col in new_history_cols:\n",
    "    if col not in final_cols:\n",
    "        final_cols.append(col)\n",
    "\n",
    "# Re-save enriched data\n",
    "available_cols = [c for c in final_cols if c in interactions.columns]\n",
    "enriched_interactions = interactions[available_cols]\n",
    "enriched_interactions.to_parquet('data/enriched_interactions.parquet', index=False)\n",
    "print(\"Saved enriched data with User History.\")\n",
    "print(f\"Final columns: {enriched_interactions.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t4rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
